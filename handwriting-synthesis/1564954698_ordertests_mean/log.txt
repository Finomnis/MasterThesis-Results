/cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])

new run with parameters:
{'attention_mixture_components': 10,
 'batch_size': 32,
 'batch_sizes': [32, 64, 64],
 'beta1_decay': 0.9,
 'beta1_decays': [0.9, 0.9, 0.9],
 'checkpoint_dir': 'checkpoints/1564954698_ordertests_mean',
 'early_stopping_steps': 1500,
 'enable_parameter_averaging': False,
 'grad_clip': 10,
 'keep_prob_scalar': 1.0,
 'learning_rate': 0.0001,
 'learning_rates': [0.0001, 5e-05, 2e-05],
 'log_dir': 'logs',
 'log_interval': 20,
 'logging_level': 20,
 'loss_averaging_window': 100,
 'lstm_size': 400,
 'min_steps_to_checkpoint': 2000,
 'num_restarts': 2,
 'num_training_steps': 100000,
 'optimizer': 'rms',
 'output_mixture_components': 20,
 'output_units': 121,
 'patiences': [1500, 1000, 500],
 'prediction_dir': 'predictions/1564954698_ordertests_mean',
 'reader': <__main__.DataReader object at 0x7f50cf6fafd0>,
 'regularization_constant': 0.0,
 'restart_idx': 0,
 'validation_batch_size': 32,
 'warm_start_init_step': 0}
WARNING:tensorflow:From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From rnn.py:196: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From rnn.py:196: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From /cluster/yn87erow/thesis/ext/handwriting-synthesis/rnn_cell.py:80: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.
From /cluster/yn87erow/thesis/ext/handwriting-synthesis/rnn_cell.py:80: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.
WARNING:tensorflow:From /cluster/yn87erow/thesis/ext/handwriting-synthesis/rnn_cell.py:140: MultivariateNormalFullCovariance.__init__ (from tensorflow.contrib.distributions.python.ops.mvn_full_covariance) is deprecated and will be removed after 2018-10-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.
From /cluster/yn87erow/thesis/ext/handwriting-synthesis/rnn_cell.py:140: MultivariateNormalFullCovariance.__init__ (from tensorflow.contrib.distributions.python.ops.mvn_full_covariance) is deprecated and will be removed after 2018-10-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.
WARNING:tensorflow:From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/mvn_full_covariance.py:195: MultivariateNormalTriL.__init__ (from tensorflow.contrib.distributions.python.ops.mvn_tril) is deprecated and will be removed after 2018-10-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.
From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/mvn_full_covariance.py:195: MultivariateNormalTriL.__init__ (from tensorflow.contrib.distributions.python.ops.mvn_tril) is deprecated and will be removed after 2018-10-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.
WARNING:tensorflow:From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/mvn_tril.py:222: MultivariateNormalLinearOperator.__init__ (from tensorflow.contrib.distributions.python.ops.mvn_linear_operator) is deprecated and will be removed after 2018-10-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.
From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/mvn_tril.py:222: MultivariateNormalLinearOperator.__init__ (from tensorflow.contrib.distributions.python.ops.mvn_linear_operator) is deprecated and will be removed after 2018-10-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.
WARNING:tensorflow:From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/mvn_linear_operator.py:199: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/mvn_linear_operator.py:199: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
WARNING:tensorflow:From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/python/ops/distributions/normal.py:160: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/python/ops/distributions/normal.py:160: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
WARNING:tensorflow:From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/mvn_linear_operator.py:201: AffineLinearOperator.__init__ (from tensorflow.contrib.distributions.python.ops.bijectors.affine_linear_operator) is deprecated and will be removed after 2018-10-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.
From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/mvn_linear_operator.py:201: AffineLinearOperator.__init__ (from tensorflow.contrib.distributions.python.ops.bijectors.affine_linear_operator) is deprecated and will be removed after 2018-10-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.
WARNING:tensorflow:From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/bijectors/affine_linear_operator.py:158: _DistributionShape.__init__ (from tensorflow.contrib.distributions.python.ops.shape) is deprecated and will be removed after 2018-10-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.
From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/bijectors/affine_linear_operator.py:158: _DistributionShape.__init__ (from tensorflow.contrib.distributions.python.ops.shape) is deprecated and will be removed after 2018-10-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.
WARNING:tensorflow:From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/mvn_linear_operator.py:205: TransformedDistribution.__init__ (from tensorflow.python.ops.distributions.transformed_distribution) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/mvn_linear_operator.py:205: TransformedDistribution.__init__ (from tensorflow.python.ops.distributions.transformed_distribution) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
WARNING:tensorflow:From /cluster/yn87erow/thesis/ext/handwriting-synthesis/rnn_cell.py:141: Bernoulli.__init__ (from tensorflow.python.ops.distributions.bernoulli) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
From /cluster/yn87erow/thesis/ext/handwriting-synthesis/rnn_cell.py:141: Bernoulli.__init__ (from tensorflow.python.ops.distributions.bernoulli) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
WARNING:tensorflow:From /cluster/yn87erow/thesis/ext/handwriting-synthesis/rnn_cell.py:142: Categorical.__init__ (from tensorflow.python.ops.distributions.categorical) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
From /cluster/yn87erow/thesis/ext/handwriting-synthesis/rnn_cell.py:142: Categorical.__init__ (from tensorflow.python.ops.distributions.categorical) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
WARNING:tensorflow:From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/python/ops/distributions/categorical.py:278: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.random.categorical instead.
From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/python/ops/distributions/categorical.py:278: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.random.categorical instead.
all parameters:
[('Variable:0', []),
 ('Variable_1:0', []),
 ('Variable_2:0', []),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp_1:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp_1:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp:0', [30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp_1:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp_1:0', [1600]),
 ('rnn/gmm/weights/RMSProp:0', [400, 121]),
 ('rnn/gmm/weights/RMSProp_1:0', [400, 121]),
 ('rnn/gmm/biases/RMSProp:0', [121]),
 ('rnn/gmm/biases/RMSProp_1:0', [121])]
trainable parameters:
[('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121])]
trainable parameter count:
3632431
2019-08-04 23:38:33.077255: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-04 23:38:33.319697: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x3635de0 executing computations on platform CUDA. Devices:
2019-08-04 23:38:33.319769: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2019-08-04 23:38:33.344700: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100110000 Hz
2019-08-04 23:38:33.348106: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x3bf4e80 executing computations on platform Host. Devices:
2019-08-04 23:38:33.348158: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-08-04 23:38:33.350681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6705
pciBusID: 0000:03:00.0
totalMemory: 10.92GiB freeMemory: 10.77GiB
2019-08-04 23:38:33.350717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-08-04 23:38:33.353703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-04 23:38:33.353730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-08-04 23:38:33.353741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-08-04 23:38:33.353964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10479 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)
built graph
2019-08-04 23:38:34.765523: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
[[step        0]]     [[train 3.506s]]     loss: 3.89899588       [[val 1.3534s]]     loss: 3.91888547       
[[step       20]]     [[train 2.8715s]]     loss: 3.84471311       [[val 0.7128s]]     loss: 3.82785049       
[[step       40]]     [[train 2.8758s]]     loss: 3.74041195       [[val 0.7044s]]     loss: 3.72871133       
[[step       60]]     [[train 2.84s]]     loss: 3.61361818       [[val 0.7172s]]     loss: 3.6047125        
[[step       80]]     [[train 2.8592s]]     loss: 3.47041189       [[val 0.7148s]]     loss: 3.46184575       
[[step      100]]     [[train 2.8657s]]     loss: 3.34559612       [[val 0.708s]]     loss: 3.33811336       
[[step      120]]     [[train 2.9026s]]     loss: 3.13008116       [[val 0.716s]]     loss: 3.12947096       
[[step      140]]     [[train 2.9282s]]     loss: 2.92771854       [[val 0.7185s]]     loss: 2.92554896       
[[step      160]]     [[train 2.9734s]]     loss: 2.69926198       [[val 0.7114s]]     loss: 2.69611298       
[[step      180]]     [[train 2.9803s]]     loss: 2.40934506       [[val 0.711s]]     loss: 2.4057076        
[[step      200]]     [[train 2.975s]]     loss: 1.97808931       [[val 0.709s]]     loss: 1.97863311       
[[step      220]]     [[train 2.9501s]]     loss: 1.48415569       [[val 0.7077s]]     loss: 1.47805033       
[[step      240]]     [[train 2.9235s]]     loss: 0.95054819       [[val 0.7045s]]     loss: 0.94547708       
[[step      260]]     [[train 2.9157s]]     loss: 0.46123629       [[val 0.709s]]     loss: 0.45871468       
[[step      280]]     [[train 2.8937s]]     loss: 0.06471856       [[val 0.7101s]]     loss: 0.06025634       
[[step      300]]     [[train 2.8974s]]     loss: -0.18475192      [[val 0.7114s]]     loss: -0.19241451      
[[step      320]]     [[train 2.9292s]]     loss: -0.36264897      [[val 0.7096s]]     loss: -0.36757722      
[[step      340]]     [[train 2.9555s]]     loss: -0.48082161      [[val 0.7132s]]     loss: -0.48844478      
[[step      360]]     [[train 2.9419s]]     loss: -0.57448606      [[val 0.7098s]]     loss: -0.5900851       
[[step      380]]     [[train 2.9617s]]     loss: -0.64433977      [[val 0.7087s]]     loss: -0.65708714      
[[step      400]]     [[train 2.9681s]]     loss: -0.69826266      [[val 0.7087s]]     loss: -0.71155204      
[[step      420]]     [[train 2.9415s]]     loss: -0.74044692      [[val 0.7091s]]     loss: -0.7567987       
[[step      440]]     [[train 2.913s]]     loss: -0.77660376      [[val 0.7076s]]     loss: -0.79368629      
[[step      460]]     [[train 2.9296s]]     loss: -0.809398        [[val 0.7059s]]     loss: -0.82209925      
[[step      480]]     [[train 2.9359s]]     loss: -0.83788498      [[val 0.7103s]]     loss: -0.85292309      
[[step      500]]     [[train 2.9325s]]     loss: -0.85750749      [[val 0.7096s]]     loss: -0.87091375      
[[step      520]]     [[train 2.9175s]]     loss: -0.8816217       [[val 0.7084s]]     loss: -0.89254616      
[[step      540]]     [[train 2.9249s]]     loss: -0.90967476      [[val 0.7127s]]     loss: -0.91186988      
[[step      560]]     [[train 2.9057s]]     loss: -0.92723054      [[val 0.7141s]]     loss: -0.92989933      
[[step      580]]     [[train 2.8889s]]     loss: -0.94719305      [[val 0.7134s]]     loss: -0.94644152      
[[step      600]]     [[train 2.8855s]]     loss: -0.96413282      [[val 0.7147s]]     loss: -0.96488446      
[[step      620]]     [[train 2.902s]]     loss: -0.97614943      [[val 0.717s]]     loss: -0.97952346      
[[step      640]]     [[train 2.8937s]]     loss: -0.9859679       [[val 0.7152s]]     loss: -0.99503513      
[[step      660]]     [[train 2.9074s]]     loss: -0.9978532       [[val 0.7187s]]     loss: -1.00775477      
[[step      680]]     [[train 2.8996s]]     loss: -1.00739084      [[val 0.7163s]]     loss: -1.01811556      
[[step      700]]     [[train 2.9015s]]     loss: -1.02397251      [[val 0.7158s]]     loss: -1.03377533      
[[step      720]]     [[train 2.9078s]]     loss: -1.04028137      [[val 0.713s]]     loss: -1.04783077      
[[step      740]]     [[train 2.9165s]]     loss: -1.05459659      [[val 0.7136s]]     loss: -1.06356623      
[[step      760]]     [[train 2.9232s]]     loss: -1.0738134       [[val 0.7083s]]     loss: -1.07939136      
[[step      780]]     [[train 2.9289s]]     loss: -1.08417546      [[val 0.7052s]]     loss: -1.09685283      
[[step      800]]     [[train 2.9154s]]     loss: -1.09060345      [[val 0.7079s]]     loss: -1.11236232      
[[step      820]]     [[train 2.8907s]]     loss: -1.10501081      [[val 0.7129s]]     loss: -1.12648658      
[[step      840]]     [[train 2.8933s]]     loss: -1.11976313      [[val 0.7077s]]     loss: -1.14035643      
[[step      860]]     [[train 2.8776s]]     loss: -1.13133324      [[val 0.7118s]]     loss: -1.15138416      
[[step      880]]     [[train 2.8991s]]     loss: -1.14656991      [[val 0.7162s]]     loss: -1.16368262      
[[step      900]]     [[train 2.913s]]     loss: -1.16209608      [[val 0.7198s]]     loss: -1.17628326      
[[step      920]]     [[train 2.9475s]]     loss: -1.1752978       [[val 0.7195s]]     loss: -1.19060872      
[[step      940]]     [[train 2.9282s]]     loss: -1.19444665      [[val 0.7229s]]     loss: -1.20481549      
[[step      960]]     [[train 2.9248s]]     loss: -1.20841178      [[val 0.7211s]]     loss: -1.21963564      
[[step      980]]     [[train 2.923s]]     loss: -1.2298818       [[val 0.7186s]]     loss: -1.23574847      
[[step     1000]]     [[train 2.9199s]]     loss: -1.24233215      [[val 0.716s]]     loss: -1.24724354      
[[step     1020]]     [[train 2.877s]]     loss: -1.25650959      [[val 0.7142s]]     loss: -1.26352916      
[[step     1040]]     [[train 2.8953s]]     loss: -1.26837547      [[val 0.7164s]]     loss: -1.27574697      
[[step     1060]]     [[train 2.9298s]]     loss: -1.28317992      [[val 0.7162s]]     loss: -1.29206644      
[[step     1080]]     [[train 2.9194s]]     loss: -1.29601065      [[val 0.713s]]     loss: -1.30614399      
[[step     1100]]     [[train 2.9114s]]     loss: -1.31961079      [[val 0.7124s]]     loss: -1.32535979      
[[step     1120]]     [[train 2.9373s]]     loss: -1.33054205      [[val 0.7167s]]     loss: -1.33533501      
[[step     1140]]     [[train 2.9399s]]     loss: -1.33673526      [[val 0.7148s]]     loss: -1.3466547       
[[step     1160]]     [[train 2.9156s]]     loss: -1.34625833      [[val 0.7147s]]     loss: -1.35921221      
[[step     1180]]     [[train 2.9049s]]     loss: -1.35766028      [[val 0.7177s]]     loss: -1.36969828      
[[step     1200]]     [[train 2.9118s]]     loss: -1.36441239      [[val 0.7184s]]     loss: -1.37905205      
[[step     1220]]     [[train 2.9109s]]     loss: -1.38017295      [[val 0.7131s]]     loss: -1.39549079      
[[step     1240]]     [[train 2.905s]]     loss: -1.39911765      [[val 0.7118s]]     loss: -1.41338372      
[[step     1260]]     [[train 2.8948s]]     loss: -1.41964135      [[val 0.7114s]]     loss: -1.42973844      
[[step     1280]]     [[train 2.8976s]]     loss: -1.42873966      [[val 0.7161s]]     loss: -1.44353776      
[[step     1300]]     [[train 2.9234s]]     loss: -1.44892573      [[val 0.714s]]     loss: -1.46268776      
[[step     1320]]     [[train 2.9295s]]     loss: -1.46655692      [[val 0.7177s]]     loss: -1.47488228      
[[step     1340]]     [[train 2.9307s]]     loss: -1.48247772      [[val 0.7198s]]     loss: -1.49113389      
[[step     1360]]     [[train 2.9285s]]     loss: -1.4960324       [[val 0.723s]]     loss: -1.50704255      
[[step     1380]]     [[train 2.9367s]]     loss: -1.51763075      [[val 0.7209s]]     loss: -1.5216757       
[[step     1400]]     [[train 2.9072s]]     loss: -1.52740325      [[val 0.7204s]]     loss: -1.53616223      
[[step     1420]]     [[train 2.9055s]]     loss: -1.5414118       [[val 0.718s]]     loss: -1.55158612      
[[step     1440]]     [[train 2.8889s]]     loss: -1.55301401      [[val 0.7163s]]     loss: -1.56686229      
[[step     1460]]     [[train 2.897s]]     loss: -1.56856598      [[val 0.7134s]]     loss: -1.575985        
[[step     1480]]     [[train 2.903s]]     loss: -1.57663663      [[val 0.7092s]]     loss: -1.59339918      
[[step     1500]]     [[train 2.9251s]]     loss: -1.58698773      [[val 0.7113s]]     loss: -1.60226662      
[[step     1520]]     [[train 2.8888s]]     loss: -1.59628609      [[val 0.7063s]]     loss: -1.61298323      
[[step     1540]]     [[train 2.9238s]]     loss: -1.60730265      [[val 0.708s]]     loss: -1.62033833      
[[step     1560]]     [[train 2.9079s]]     loss: -1.6131236       [[val 0.71s]]     loss: -1.63038598      
[[step     1580]]     [[train 2.9253s]]     loss: -1.62412085      [[val 0.7073s]]     loss: -1.63822312      
[[step     1600]]     [[train 2.9192s]]     loss: -1.64523573      [[val 0.7081s]]     loss: -1.64721777      
[[step     1620]]     [[train 2.9363s]]     loss: -1.65484861      [[val 0.7159s]]     loss: -1.65493793      
[[step     1640]]     [[train 2.9048s]]     loss: -1.66419811      [[val 0.716s]]     loss: -1.66258448      
[[step     1660]]     [[train 2.9114s]]     loss: -1.67449955      [[val 0.7092s]]     loss: -1.67207643      
[[step     1680]]     [[train 2.8926s]]     loss: -1.68380588      [[val 0.7128s]]     loss: -1.68282739      
[[step     1700]]     [[train 2.8801s]]     loss: -1.68796499      [[val 0.7059s]]     loss: -1.69287029      
[[step     1720]]     [[train 2.8872s]]     loss: -1.69620363      [[val 0.7107s]]     loss: -1.70263956      
[[step     1740]]     [[train 2.887s]]     loss: -1.70315936      [[val 0.7064s]]     loss: -1.71353087      
[[step     1760]]     [[train 2.8877s]]     loss: -1.70878392      [[val 0.712s]]     loss: -1.72122543      
[[step     1780]]     [[train 2.895s]]     loss: -1.71330787      [[val 0.7126s]]     loss: -1.72704679      
[[step     1800]]     [[train 2.9029s]]     loss: -1.71556588      [[val 0.7169s]]     loss: -1.73370182      
[[step     1820]]     [[train 2.9133s]]     loss: -1.72176133      [[val 0.7086s]]     loss: -1.74431236      
[[step     1840]]     [[train 2.9233s]]     loss: -1.72403658      [[val 0.7106s]]     loss: -1.74503728      
[[step     1860]]     [[train 2.9278s]]     loss: -1.73352571      [[val 0.7109s]]     loss: -1.75101817      
[[step     1880]]     [[train 2.9245s]]     loss: -1.72920017      [[val 0.7133s]]     loss: -1.74092898      
[[step     1900]]     [[train 2.9404s]]     loss: -1.72956891      [[val 0.7065s]]     loss: -1.74360514      
[[step     1920]]     [[train 2.9398s]]     loss: -1.73650779      [[val 0.7075s]]     loss: -1.74734356      
[[step     1940]]     [[train 2.9437s]]     loss: -1.75219101      [[val 0.7092s]]     loss: -1.75906151      
[[step     1960]]     [[train 2.9412s]]     loss: -1.76279643      [[val 0.71s]]     loss: -1.77091741      
[[step     1980]]     [[train 2.9443s]]     loss: -1.78247614      [[val 0.7114s]]     loss: -1.7906516       
[[step     2000]]     [[train 2.9254s]]     loss: -1.79741968      [[val 0.7188s]]     loss: -1.80040266      
[[step     2020]]     [[train 2.9321s]]     loss: -1.80255679      [[val 0.7163s]]     loss: -1.80672813      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2040]]     [[train 2.9119s]]     loss: -1.80832331      [[val 0.718s]]     loss: -1.8123728       
saving model to checkpoints/1564954698_ordertests_mean/model
WARNING:tensorflow:From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
[[step     2060]]     [[train 2.9299s]]     loss: -1.8115537       [[val 0.7192s]]     loss: -1.8150352       
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2080]]     [[train 2.9013s]]     loss: -1.81797608      [[val 0.712s]]     loss: -1.82339814      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2100]]     [[train 2.9057s]]     loss: -1.81933841      [[val 0.7146s]]     loss: -1.8309217       
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2120]]     [[train 2.899s]]     loss: -1.82905032      [[val 0.7159s]]     loss: -1.83886304      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2140]]     [[train 2.923s]]     loss: -1.83058536      [[val 0.7118s]]     loss: -1.84437024      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2160]]     [[train 2.9196s]]     loss: -1.83284876      [[val 0.7107s]]     loss: -1.85247527      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2180]]     [[train 2.9104s]]     loss: -1.84399068      [[val 0.711s]]     loss: -1.85810921      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2200]]     [[train 2.9243s]]     loss: -1.85231659      [[val 0.7056s]]     loss: -1.86203037      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2220]]     [[train 2.9274s]]     loss: -1.85657286      [[val 0.7073s]]     loss: -1.86610649      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2240]]     [[train 2.9078s]]     loss: -1.86413702      [[val 0.7093s]]     loss: -1.87291086      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2260]]     [[train 2.895s]]     loss: -1.8728793       [[val 0.7077s]]     loss: -1.87345474      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2280]]     [[train 2.8992s]]     loss: -1.87251085      [[val 0.7107s]]     loss: -1.88002175      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2300]]     [[train 2.8864s]]     loss: -1.88038605      [[val 0.7116s]]     loss: -1.88584345      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2320]]     [[train 2.8773s]]     loss: -1.88748315      [[val 0.7062s]]     loss: -1.89447414      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2340]]     [[train 2.92s]]     loss: -1.8894651       [[val 0.708s]]     loss: -1.89894807      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2360]]     [[train 2.9376s]]     loss: -1.89358376      [[val 0.71s]]     loss: -1.90721264      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2380]]     [[train 2.9466s]]     loss: -1.89822428      [[val 0.7134s]]     loss: -1.90753644      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2400]]     [[train 2.9235s]]     loss: -1.8962421       [[val 0.7112s]]     loss: -1.90911152      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2420]]     [[train 2.9126s]]     loss: -1.89305553      [[val 0.7166s]]     loss: -1.89941511      
[[step     2440]]     [[train 2.8936s]]     loss: -1.89582851      [[val 0.7143s]]     loss: -1.90157062      
[[step     2460]]     [[train 2.8906s]]     loss: -1.89411941      [[val 0.7087s]]     loss: -1.90519478      
[[step     2480]]     [[train 2.8987s]]     loss: -1.89631889      [[val 0.709s]]     loss: -1.90892603      
[[step     2500]]     [[train 2.9215s]]     loss: -1.90807017      [[val 0.7069s]]     loss: -1.91484799      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2520]]     [[train 2.9584s]]     loss: -1.92016322      [[val 0.7044s]]     loss: -1.92898691      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2540]]     [[train 2.9586s]]     loss: -1.9311156       [[val 0.7034s]]     loss: -1.93602647      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2560]]     [[train 2.9452s]]     loss: -1.94074944      [[val 0.706s]]     loss: -1.93828319      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2580]]     [[train 2.9671s]]     loss: -1.9546203       [[val 0.7008s]]     loss: -1.94630161      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2600]]     [[train 2.9718s]]     loss: -1.95225767      [[val 0.7081s]]     loss: -1.95173305      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2620]]     [[train 2.9591s]]     loss: -1.9491307       [[val 0.71s]]     loss: -1.95168764      
[[step     2640]]     [[train 2.9535s]]     loss: -1.94748881      [[val 0.7102s]]     loss: -1.95456101      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2660]]     [[train 2.9383s]]     loss: -1.94737965      [[val 0.7069s]]     loss: -1.9558447       
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2680]]     [[train 2.9178s]]     loss: -1.94370013      [[val 0.7109s]]     loss: -1.95726249      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2700]]     [[train 2.9084s]]     loss: -1.94728098      [[val 0.709s]]     loss: -1.95801251      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2720]]     [[train 2.8934s]]     loss: -1.95712963      [[val 0.708s]]     loss: -1.96715183      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2740]]     [[train 2.9062s]]     loss: -1.96326179      [[val 0.709s]]     loss: -1.96979632      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2760]]     [[train 2.9275s]]     loss: -1.96401921      [[val 0.7146s]]     loss: -1.97373662      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2780]]     [[train 2.9167s]]     loss: -1.97073692      [[val 0.7109s]]     loss: -1.98007233      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2800]]     [[train 2.9088s]]     loss: -1.97664971      [[val 0.7102s]]     loss: -1.98300526      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2820]]     [[train 2.9264s]]     loss: -1.97869985      [[val 0.7107s]]     loss: -1.98521183      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2840]]     [[train 2.8944s]]     loss: -1.98616492      [[val 0.7126s]]     loss: -1.98821226      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2860]]     [[train 2.8837s]]     loss: -1.99862435      [[val 0.7105s]]     loss: -1.99512856      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2880]]     [[train 2.8899s]]     loss: -2.00451681      [[val 0.7108s]]     loss: -1.99823259      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2900]]     [[train 2.9157s]]     loss: -2.00735925      [[val 0.7115s]]     loss: -2.00208491      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2920]]     [[train 2.9232s]]     loss: -2.00581904      [[val 0.7099s]]     loss: -2.00233672      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2940]]     [[train 2.9554s]]     loss: -2.00414472      [[val 0.7123s]]     loss: -2.00375564      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2960]]     [[train 2.9839s]]     loss: -2.00175025      [[val 0.7109s]]     loss: -2.00720296      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     2980]]     [[train 2.9699s]]     loss: -2.0022273       [[val 0.7082s]]     loss: -2.00647868      
[[step     3000]]     [[train 2.9368s]]     loss: -2.00537659      [[val 0.7117s]]     loss: -2.00913109      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3020]]     [[train 2.901s]]     loss: -2.01615158      [[val 0.7123s]]     loss: -2.01755445      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3040]]     [[train 2.8627s]]     loss: -2.02140981      [[val 0.7082s]]     loss: -2.02067094      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3060]]     [[train 2.8478s]]     loss: -2.0125174       [[val 0.7106s]]     loss: -2.0140223       
[[step     3080]]     [[train 2.8557s]]     loss: -2.01014447      [[val 0.7215s]]     loss: -2.01455523      
[[step     3100]]     [[train 2.8691s]]     loss: -2.0139439       [[val 0.7136s]]     loss: -2.02003825      
[[step     3120]]     [[train 2.8811s]]     loss: -2.01696954      [[val 0.7133s]]     loss: -2.02254057      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3140]]     [[train 2.9069s]]     loss: -2.01726055      [[val 0.715s]]     loss: -2.02655553      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3160]]     [[train 2.9162s]]     loss: -2.03336555      [[val 0.7148s]]     loss: -2.03848074      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3180]]     [[train 2.9376s]]     loss: -2.0384079       [[val 0.7136s]]     loss: -2.04380125      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3200]]     [[train 2.9369s]]     loss: -2.04796921      [[val 0.7209s]]     loss: -2.04334617      
[[step     3220]]     [[train 2.935s]]     loss: -2.04905961      [[val 0.7225s]]     loss: -2.04388057      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3240]]     [[train 2.9296s]]     loss: -2.05175909      [[val 0.7173s]]     loss: -2.04637301      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3260]]     [[train 2.9141s]]     loss: -2.05369362      [[val 0.7164s]]     loss: -2.04714818      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3280]]     [[train 2.9268s]]     loss: -2.05844827      [[val 0.7115s]]     loss: -2.05085142      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3300]]     [[train 2.9103s]]     loss: -2.05084532      [[val 0.7074s]]     loss: -2.05333385      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3320]]     [[train 2.9026s]]     loss: -2.05009687      [[val 0.7094s]]     loss: -2.05334657      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3340]]     [[train 2.8894s]]     loss: -2.05162012      [[val 0.707s]]     loss: -2.05435457      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3360]]     [[train 2.9117s]]     loss: -2.055349        [[val 0.7053s]]     loss: -2.05400122      
[[step     3380]]     [[train 2.9115s]]     loss: -2.05082487      [[val 0.7063s]]     loss: -2.05866405      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3400]]     [[train 2.9141s]]     loss: -2.05907172      [[val 0.7081s]]     loss: -2.06243163      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3420]]     [[train 2.9144s]]     loss: -2.05815882      [[val 0.7047s]]     loss: -2.06348403      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3440]]     [[train 2.9208s]]     loss: -2.06048026      [[val 0.7112s]]     loss: -2.06645765      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3460]]     [[train 2.8959s]]     loss: -2.05265576      [[val 0.7116s]]     loss: -2.06828267      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3480]]     [[train 2.885s]]     loss: -2.05068529      [[val 0.7044s]]     loss: -2.06083267      
[[step     3500]]     [[train 2.8921s]]     loss: -2.04233289      [[val 0.7045s]]     loss: -2.05397732      
[[step     3520]]     [[train 2.8998s]]     loss: -2.04393189      [[val 0.7027s]]     loss: -2.05539449      
[[step     3540]]     [[train 2.9232s]]     loss: -2.04482177      [[val 0.7035s]]     loss: -2.05379378      
[[step     3560]]     [[train 2.9022s]]     loss: -2.05575093      [[val 0.7053s]]     loss: -2.05605211      
[[step     3580]]     [[train 2.8865s]]     loss: -2.07076375      [[val 0.7123s]]     loss: -2.06563038      
[[step     3600]]     [[train 2.8966s]]     loss: -2.07955943      [[val 0.709s]]     loss: -2.07971901      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3620]]     [[train 2.9092s]]     loss: -2.08494968      [[val 0.7095s]]     loss: -2.08605347      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3640]]     [[train 2.9093s]]     loss: -2.0898217       [[val 0.7075s]]     loss: -2.08990905      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3660]]     [[train 2.9271s]]     loss: -2.0883895       [[val 0.7094s]]     loss: -2.09128862      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3680]]     [[train 2.9216s]]     loss: -2.0811696       [[val 0.7075s]]     loss: -2.09223433      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3700]]     [[train 2.9174s]]     loss: -2.08257207      [[val 0.7136s]]     loss: -2.08783119      
[[step     3720]]     [[train 2.8987s]]     loss: -2.08297671      [[val 0.7154s]]     loss: -2.08367568      
[[step     3740]]     [[train 2.8763s]]     loss: -2.08154996      [[val 0.717s]]     loss: -2.08488963      
[[step     3760]]     [[train 2.8821s]]     loss: -2.08883924      [[val 0.7113s]]     loss: -2.08944191      
[[step     3780]]     [[train 2.9222s]]     loss: -2.0964193       [[val 0.7176s]]     loss: -2.09320761      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3800]]     [[train 2.9163s]]     loss: -2.09822963      [[val 0.7129s]]     loss: -2.09537905      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3820]]     [[train 2.9428s]]     loss: -2.10034002      [[val 0.7168s]]     loss: -2.09844795      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3840]]     [[train 2.9663s]]     loss: -2.1066757       [[val 0.7163s]]     loss: -2.10289359      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3860]]     [[train 2.9675s]]     loss: -2.10684035      [[val 0.7186s]]     loss: -2.10332943      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     3880]]     [[train 2.9314s]]     loss: -2.10611782      [[val 0.7181s]]     loss: -2.10086474      
[[step     3900]]     [[train 2.9527s]]     loss: -2.10687213      [[val 0.7139s]]     loss: -2.09950667      
[[step     3920]]     [[train 2.9261s]]     loss: -2.11083702      [[val 0.7082s]]     loss: -2.10007788      
[[step     3940]]     [[train 2.9093s]]     loss: -2.11165177      [[val 0.71s]]     loss: -2.09842446      
[[step     3960]]     [[train 2.9235s]]     loss: -2.10923392      [[val 0.7137s]]     loss: -2.10068457      
[[step     3980]]     [[train 2.9098s]]     loss: -2.11401204      [[val 0.7133s]]     loss: -2.10212355      
[[step     4000]]     [[train 2.9016s]]     loss: -2.11757569      [[val 0.7194s]]     loss: -2.10858674      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     4020]]     [[train 2.9261s]]     loss: -2.11721394      [[val 0.7231s]]     loss: -2.11703881      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     4040]]     [[train 2.9263s]]     loss: -2.11594994      [[val 0.7201s]]     loss: -2.11933669      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     4060]]     [[train 2.9182s]]     loss: -2.12093435      [[val 0.721s]]     loss: -2.12270198      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     4080]]     [[train 2.9268s]]     loss: -2.12143139      [[val 0.7157s]]     loss: -2.12661211      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     4100]]     [[train 2.9457s]]     loss: -2.12186697      [[val 0.7211s]]     loss: -2.12582648      
[[step     4120]]     [[train 2.9074s]]     loss: -2.12262115      [[val 0.7176s]]     loss: -2.1249019       
[[step     4140]]     [[train 2.9218s]]     loss: -2.12546171      [[val 0.72s]]     loss: -2.12506933      
[[step     4160]]     [[train 2.9062s]]     loss: -2.11685991      [[val 0.7141s]]     loss: -2.10825118      
[[step     4180]]     [[train 2.9168s]]     loss: -2.11898591      [[val 0.7182s]]     loss: -2.10819693      
[[step     4200]]     [[train 2.8914s]]     loss: -2.12273268      [[val 0.712s]]     loss: -2.11392643      
[[step     4220]]     [[train 2.9134s]]     loss: -2.12705038      [[val 0.7177s]]     loss: -2.11389024      
[[step     4240]]     [[train 2.9307s]]     loss: -2.12844197      [[val 0.7173s]]     loss: -2.11982559      
[[step     4260]]     [[train 2.935s]]     loss: -2.14244123      [[val 0.7211s]]     loss: -2.13688308      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     4280]]     [[train 2.9476s]]     loss: -2.14210691      [[val 0.7162s]]     loss: -2.14034063      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     4300]]     [[train 2.9434s]]     loss: -2.13953702      [[val 0.7172s]]     loss: -2.13823485      
[[step     4320]]     [[train 2.9622s]]     loss: -2.14270717      [[val 0.7109s]]     loss: -2.14142667      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     4340]]     [[train 2.9281s]]     loss: -2.14469545      [[val 0.7108s]]     loss: -2.14368525      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     4360]]     [[train 2.9547s]]     loss: -2.14583209      [[val 0.7113s]]     loss: -2.14213833      
[[step     4380]]     [[train 2.937s]]     loss: -2.14625916      [[val 0.7167s]]     loss: -2.14359561      
[[step     4400]]     [[train 2.9323s]]     loss: -2.14887284      [[val 0.7171s]]     loss: -2.14979068      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     4420]]     [[train 2.9072s]]     loss: -2.14764394      [[val 0.7181s]]     loss: -2.15226547      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     4440]]     [[train 2.9203s]]     loss: -2.15057175      [[val 0.7193s]]     loss: -2.1517006       
[[step     4460]]     [[train 2.9099s]]     loss: -2.15160653      [[val 0.7207s]]     loss: -2.15340664      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     4480]]     [[train 2.9332s]]     loss: -2.16164037      [[val 0.7198s]]     loss: -2.15536849      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     4500]]     [[train 2.936s]]     loss: -2.16664607      [[val 0.7197s]]     loss: -2.15092108      
[[step     4520]]     [[train 2.9559s]]     loss: -2.16482263      [[val 0.7245s]]     loss: -2.14493403      
[[step     4540]]     [[train 2.9515s]]     loss: -2.16128623      [[val 0.7287s]]     loss: -2.14620045      
[[step     4560]]     [[train 2.9587s]]     loss: -2.16139279      [[val 0.7255s]]     loss: -2.15237402      
[[step     4580]]     [[train 2.9469s]]     loss: -2.1587257       [[val 0.7309s]]     loss: -2.15073296      
[[step     4600]]     [[train 2.9369s]]     loss: -2.16132123      [[val 0.7256s]]     loss: -2.15588993      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     4620]]     [[train 2.9253s]]     loss: -2.16402749      [[val 0.7187s]]     loss: -2.16516273      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     4640]]     [[train 2.9109s]]     loss: -2.17173924      [[val 0.713s]]     loss: -2.16390182      
[[step     4660]]     [[train 2.8816s]]     loss: -2.17225364      [[val 0.7102s]]     loss: -2.16358759      
[[step     4680]]     [[train 2.8569s]]     loss: -2.16957952      [[val 0.7052s]]     loss: -2.16712338      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     4700]]     [[train 2.8678s]]     loss: -2.16417283      [[val 0.7087s]]     loss: -2.16640079      
[[step     4720]]     [[train 2.8538s]]     loss: -2.16708521      [[val 0.716s]]     loss: -2.16124634      
[[step     4740]]     [[train 2.8885s]]     loss: -2.16208816      [[val 0.7158s]]     loss: -2.16184182      
[[step     4760]]     [[train 2.9013s]]     loss: -2.16052103      [[val 0.7209s]]     loss: -2.16217875      
[[step     4780]]     [[train 2.9291s]]     loss: -2.16189261      [[val 0.7211s]]     loss: -2.16248473      
[[step     4800]]     [[train 2.9426s]]     loss: -2.17318226      [[val 0.7199s]]     loss: -2.16350695      
[[step     4820]]     [[train 2.9633s]]     loss: -2.17172303      [[val 0.7138s]]     loss: -2.16678508      
[[step     4840]]     [[train 2.949s]]     loss: -2.17311579      [[val 0.712s]]     loss: -2.1672164       
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     4860]]     [[train 2.9644s]]     loss: -2.18185577      [[val 0.712s]]     loss: -2.17164418      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     4880]]     [[train 2.9462s]]     loss: -2.18898486      [[val 0.708s]]     loss: -2.17142005      
[[step     4900]]     [[train 2.9356s]]     loss: -2.1812869       [[val 0.7107s]]     loss: -2.17023188      
[[step     4920]]     [[train 2.9199s]]     loss: -2.18816343      [[val 0.7113s]]     loss: -2.16825055      
[[step     4940]]     [[train 2.9056s]]     loss: -2.18962979      [[val 0.7167s]]     loss: -2.16958909      
[[step     4960]]     [[train 2.8812s]]     loss: -2.1784307       [[val 0.7138s]]     loss: -2.15914017      
[[step     4980]]     [[train 2.9022s]]     loss: -2.17218638      [[val 0.7211s]]     loss: -2.16037369      
[[step     5000]]     [[train 2.9231s]]     loss: -2.17707425      [[val 0.7213s]]     loss: -2.16603422      
[[step     5020]]     [[train 2.9324s]]     loss: -2.17405727      [[val 0.7207s]]     loss: -2.16800423      
[[step     5040]]     [[train 2.9406s]]     loss: -2.1790604       [[val 0.7225s]]     loss: -2.17288392      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     5060]]     [[train 2.9532s]]     loss: -2.19197335      [[val 0.7217s]]     loss: -2.18503717      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     5080]]     [[train 2.9378s]]     loss: -2.19554631      [[val 0.7153s]]     loss: -2.18384062      
[[step     5100]]     [[train 2.9393s]]     loss: -2.19884822      [[val 0.7166s]]     loss: -2.18348303      
[[step     5120]]     [[train 2.8995s]]     loss: -2.20557432      [[val 0.7159s]]     loss: -2.18828181      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     5140]]     [[train 2.8801s]]     loss: -2.20474036      [[val 0.7127s]]     loss: -2.18780839      
[[step     5160]]     [[train 2.8692s]]     loss: -2.20386541      [[val 0.716s]]     loss: -2.18052356      
[[step     5180]]     [[train 2.8915s]]     loss: -2.20598887      [[val 0.7153s]]     loss: -2.18733376      
[[step     5200]]     [[train 2.8922s]]     loss: -2.20350025      [[val 0.7108s]]     loss: -2.19074125      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     5220]]     [[train 2.9428s]]     loss: -2.20078119      [[val 0.715s]]     loss: -2.18877642      
[[step     5240]]     [[train 2.9773s]]     loss: -2.20352821      [[val 0.7123s]]     loss: -2.19242047      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     5260]]     [[train 2.9813s]]     loss: -2.20354636      [[val 0.7106s]]     loss: -2.1993283       
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     5280]]     [[train 2.9687s]]     loss: -2.20444451      [[val 0.7154s]]     loss: -2.1972565       
[[step     5300]]     [[train 2.9373s]]     loss: -2.20469028      [[val 0.7141s]]     loss: -2.19389604      
[[step     5320]]     [[train 2.926s]]     loss: -2.20550554      [[val 0.7132s]]     loss: -2.19327166      
[[step     5340]]     [[train 2.8927s]]     loss: -2.2133224       [[val 0.7124s]]     loss: -2.19538324      
[[step     5360]]     [[train 2.9173s]]     loss: -2.21089325      [[val 0.7102s]]     loss: -2.19734138      
[[step     5380]]     [[train 2.922s]]     loss: -2.21117793      [[val 0.7056s]]     loss: -2.19816374      
[[step     5400]]     [[train 2.9111s]]     loss: -2.20805774      [[val 0.7089s]]     loss: -2.1979233       
[[step     5420]]     [[train 2.9307s]]     loss: -2.20244056      [[val 0.703s]]     loss: -2.19570611      
[[step     5440]]     [[train 2.9729s]]     loss: -2.19562918      [[val 0.7111s]]     loss: -2.19231349      
[[step     5460]]     [[train 2.9429s]]     loss: -2.19731681      [[val 0.7092s]]     loss: -2.19349378      
[[step     5480]]     [[train 2.9268s]]     loss: -2.20599476      [[val 0.7172s]]     loss: -2.19470532      
[[step     5500]]     [[train 2.9429s]]     loss: -2.21343422      [[val 0.7166s]]     loss: -2.19651727      
[[step     5520]]     [[train 2.9185s]]     loss: -2.22117925      [[val 0.7215s]]     loss: -2.20093493      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     5540]]     [[train 2.8922s]]     loss: -2.22064914      [[val 0.7175s]]     loss: -2.19892884      
[[step     5560]]     [[train 2.9088s]]     loss: -2.21676975      [[val 0.7154s]]     loss: -2.19750196      
[[step     5580]]     [[train 2.936s]]     loss: -2.2087184       [[val 0.7078s]]     loss: -2.19264307      
[[step     5600]]     [[train 2.9386s]]     loss: -2.20916197      [[val 0.7092s]]     loss: -2.19026136      
[[step     5620]]     [[train 2.9353s]]     loss: -2.20798325      [[val 0.7075s]]     loss: -2.19581293      
[[step     5640]]     [[train 2.9581s]]     loss: -2.2105866       [[val 0.6988s]]     loss: -2.20175208      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     5660]]     [[train 2.9542s]]     loss: -2.21703782      [[val 0.7039s]]     loss: -2.200338        
[[step     5680]]     [[train 2.9098s]]     loss: -2.226229        [[val 0.7042s]]     loss: -2.20867877      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     5700]]     [[train 2.9151s]]     loss: -2.22278531      [[val 0.7046s]]     loss: -2.21353352      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     5720]]     [[train 2.9346s]]     loss: -2.2242729       [[val 0.7063s]]     loss: -2.21482631      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     5740]]     [[train 2.9207s]]     loss: -2.22728385      [[val 0.7151s]]     loss: -2.2124097       
[[step     5760]]     [[train 2.9128s]]     loss: -2.23042285      [[val 0.7152s]]     loss: -2.21691557      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     5780]]     [[train 2.9222s]]     loss: -2.22451063      [[val 0.7117s]]     loss: -2.21457269      
[[step     5800]]     [[train 2.9367s]]     loss: -2.22949126      [[val 0.7186s]]     loss: -2.21958487      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     5820]]     [[train 2.9486s]]     loss: -2.23245471      [[val 0.7186s]]     loss: -2.21497557      
[[step     5840]]     [[train 2.9482s]]     loss: -2.23498921      [[val 0.7139s]]     loss: -2.21623323      
[[step     5860]]     [[train 2.943s]]     loss: -2.23669899      [[val 0.716s]]     loss: -2.21507815      
[[step     5880]]     [[train 2.9581s]]     loss: -2.23716342      [[val 0.7239s]]     loss: -2.21434411      
[[step     5900]]     [[train 2.9408s]]     loss: -2.23706391      [[val 0.7161s]]     loss: -2.21235448      
[[step     5920]]     [[train 2.9153s]]     loss: -2.24259856      [[val 0.7178s]]     loss: -2.21543471      
[[step     5940]]     [[train 2.8878s]]     loss: -2.23936111      [[val 0.7182s]]     loss: -2.21690139      
[[step     5960]]     [[train 2.8964s]]     loss: -2.24024269      [[val 0.7197s]]     loss: -2.21699342      
[[step     5980]]     [[train 2.9052s]]     loss: -2.24323998      [[val 0.7088s]]     loss: -2.22041583      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     6000]]     [[train 2.8913s]]     loss: -2.23559286      [[val 0.7123s]]     loss: -2.21376148      
[[step     6020]]     [[train 2.909s]]     loss: -2.23047294      [[val 0.7141s]]     loss: -2.21616289      
[[step     6040]]     [[train 2.9381s]]     loss: -2.23001096      [[val 0.713s]]     loss: -2.21519246      
[[step     6060]]     [[train 2.9557s]]     loss: -2.22886524      [[val 0.7105s]]     loss: -2.21618933      
[[step     6080]]     [[train 2.9582s]]     loss: -2.23214933      [[val 0.7171s]]     loss: -2.21646751      
[[step     6100]]     [[train 2.9516s]]     loss: -2.24452759      [[val 0.7168s]]     loss: -2.22212171      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     6120]]     [[train 2.9516s]]     loss: -2.24869719      [[val 0.7095s]]     loss: -2.22360217      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     6140]]     [[train 2.9337s]]     loss: -2.25257689      [[val 0.7163s]]     loss: -2.22495073      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     6160]]     [[train 2.9104s]]     loss: -2.25897392      [[val 0.7155s]]     loss: -2.22685367      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     6180]]     [[train 2.8908s]]     loss: -2.25785628      [[val 0.7153s]]     loss: -2.22890528      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     6200]]     [[train 2.9057s]]     loss: -2.25665063      [[val 0.7116s]]     loss: -2.22949267      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     6220]]     [[train 2.8944s]]     loss: -2.2629002       [[val 0.7119s]]     loss: -2.23109301      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     6240]]     [[train 2.9108s]]     loss: -2.26227849      [[val 0.7086s]]     loss: -2.23172763      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     6260]]     [[train 2.934s]]     loss: -2.26084875      [[val 0.7085s]]     loss: -2.23188323      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     6280]]     [[train 2.959s]]     loss: -2.25655893      [[val 0.7108s]]     loss: -2.23465199      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     6300]]     [[train 2.9752s]]     loss: -2.25494017      [[val 0.7094s]]     loss: -2.2338932       
[[step     6320]]     [[train 2.9594s]]     loss: -2.24795273      [[val 0.7158s]]     loss: -2.23117918      
[[step     6340]]     [[train 2.9526s]]     loss: -2.24281871      [[val 0.7202s]]     loss: -2.23379398      
[[step     6360]]     [[train 2.9268s]]     loss: -2.24006913      [[val 0.7181s]]     loss: -2.23462794      
[[step     6380]]     [[train 2.9322s]]     loss: -2.24920323      [[val 0.7165s]]     loss: -2.23188198      
[[step     6400]]     [[train 2.9139s]]     loss: -2.26002043      [[val 0.7162s]]     loss: -2.23573857      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     6420]]     [[train 2.9241s]]     loss: -2.25959611      [[val 0.7196s]]     loss: -2.23629877      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     6440]]     [[train 2.9289s]]     loss: -2.26622577      [[val 0.7067s]]     loss: -2.23623691      
[[step     6460]]     [[train 2.9276s]]     loss: -2.26578162      [[val 0.7099s]]     loss: -2.2356613       
[[step     6480]]     [[train 2.9102s]]     loss: -2.25637405      [[val 0.7057s]]     loss: -2.2339224       
[[step     6500]]     [[train 2.9049s]]     loss: -2.24576121      [[val 0.7103s]]     loss: -2.23218372      
[[step     6520]]     [[train 2.8978s]]     loss: -2.24597109      [[val 0.7024s]]     loss: -2.23237162      
[[step     6540]]     [[train 2.906s]]     loss: -2.24948833      [[val 0.7125s]]     loss: -2.23113683      
[[step     6560]]     [[train 2.8967s]]     loss: -2.25210486      [[val 0.713s]]     loss: -2.23317762      
[[step     6580]]     [[train 2.8762s]]     loss: -2.25787286      [[val 0.7126s]]     loss: -2.23685906      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     6600]]     [[train 2.8983s]]     loss: -2.264576        [[val 0.7134s]]     loss: -2.24022346      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     6620]]     [[train 2.9128s]]     loss: -2.27182859      [[val 0.7165s]]     loss: -2.24322506      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     6640]]     [[train 2.8811s]]     loss: -2.26922676      [[val 0.7167s]]     loss: -2.24197598      
[[step     6660]]     [[train 2.8909s]]     loss: -2.26877528      [[val 0.7131s]]     loss: -2.2431171       
[[step     6680]]     [[train 2.8972s]]     loss: -2.27251069      [[val 0.7163s]]     loss: -2.23956878      
[[step     6700]]     [[train 2.9172s]]     loss: -2.26844224      [[val 0.715s]]     loss: -2.24096921      
[[step     6720]]     [[train 2.8905s]]     loss: -2.26754806      [[val 0.7135s]]     loss: -2.24126544      
[[step     6740]]     [[train 2.904s]]     loss: -2.27161093      [[val 0.7138s]]     loss: -2.24225155      
[[step     6760]]     [[train 2.9112s]]     loss: -2.27709157      [[val 0.715s]]     loss: -2.24342038      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     6780]]     [[train 2.9194s]]     loss: -2.27228575      [[val 0.7189s]]     loss: -2.24801362      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     6800]]     [[train 2.8587s]]     loss: -2.27923584      [[val 0.7154s]]     loss: -2.24823541      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     6820]]     [[train 2.8753s]]     loss: -2.28103251      [[val 0.7191s]]     loss: -2.24844472      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     6840]]     [[train 2.8696s]]     loss: -2.27758955      [[val 0.7117s]]     loss: -2.25212497      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     6860]]     [[train 2.8714s]]     loss: -2.2710684       [[val 0.7128s]]     loss: -2.25023448      
[[step     6880]]     [[train 2.8586s]]     loss: -2.27761862      [[val 0.7111s]]     loss: -2.25015898      
[[step     6900]]     [[train 2.8897s]]     loss: -2.27476882      [[val 0.7125s]]     loss: -2.24905031      
[[step     6920]]     [[train 2.8923s]]     loss: -2.2685527       [[val 0.7108s]]     loss: -2.2446618       
[[step     6940]]     [[train 2.9295s]]     loss: -2.27248533      [[val 0.7169s]]     loss: -2.2437075       
[[step     6960]]     [[train 2.9313s]]     loss: -2.27791244      [[val 0.7183s]]     loss: -2.2444259       
[[step     6980]]     [[train 2.945s]]     loss: -2.28041315      [[val 0.7175s]]     loss: -2.24536517      
[[step     7000]]     [[train 2.9302s]]     loss: -2.2867208       [[val 0.7146s]]     loss: -2.25109832      
[[step     7020]]     [[train 2.9264s]]     loss: -2.29477972      [[val 0.72s]]     loss: -2.25251458      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     7040]]     [[train 2.8816s]]     loss: -2.2924981       [[val 0.7153s]]     loss: -2.25012623      
[[step     7060]]     [[train 2.8688s]]     loss: -2.29079857      [[val 0.7157s]]     loss: -2.24759121      
[[step     7080]]     [[train 2.865s]]     loss: -2.28727851      [[val 0.7154s]]     loss: -2.24458545      
[[step     7100]]     [[train 2.8907s]]     loss: -2.2844722       [[val 0.7178s]]     loss: -2.24194932      
[[step     7120]]     [[train 2.8922s]]     loss: -2.27869609      [[val 0.7115s]]     loss: -2.24513126      
[[step     7140]]     [[train 2.9355s]]     loss: -2.27873109      [[val 0.7186s]]     loss: -2.24761711      
[[step     7160]]     [[train 2.9328s]]     loss: -2.27846079      [[val 0.7111s]]     loss: -2.2545281       
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     7180]]     [[train 2.9548s]]     loss: -2.27691715      [[val 0.7139s]]     loss: -2.25472722      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     7200]]     [[train 2.9275s]]     loss: -2.2706958       [[val 0.7197s]]     loss: -2.24808083      
[[step     7220]]     [[train 2.9403s]]     loss: -2.2708415       [[val 0.7183s]]     loss: -2.24896744      
[[step     7240]]     [[train 2.9301s]]     loss: -2.27634485      [[val 0.7141s]]     loss: -2.2513494       
[[step     7260]]     [[train 2.9206s]]     loss: -2.27882694      [[val 0.7197s]]     loss: -2.25250936      
[[step     7280]]     [[train 2.9003s]]     loss: -2.28329583      [[val 0.72s]]     loss: -2.25498413      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     7300]]     [[train 2.9012s]]     loss: -2.29298799      [[val 0.7175s]]     loss: -2.25992484      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     7320]]     [[train 2.8957s]]     loss: -2.29643429      [[val 0.7124s]]     loss: -2.2612143       
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     7340]]     [[train 2.862s]]     loss: -2.2932967       [[val 0.7142s]]     loss: -2.25978033      
[[step     7360]]     [[train 2.8772s]]     loss: -2.29530223      [[val 0.7112s]]     loss: -2.25809425      
[[step     7380]]     [[train 2.8875s]]     loss: -2.29596911      [[val 0.7142s]]     loss: -2.25902435      
[[step     7400]]     [[train 2.9043s]]     loss: -2.29176587      [[val 0.7049s]]     loss: -2.26207454      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     7420]]     [[train 2.8942s]]     loss: -2.29126743      [[val 0.7138s]]     loss: -2.26175687      
[[step     7440]]     [[train 2.934s]]     loss: -2.29912396      [[val 0.7096s]]     loss: -2.26638087      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     7460]]     [[train 2.9639s]]     loss: -2.30496904      [[val 0.7163s]]     loss: -2.26656919      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     7480]]     [[train 2.9571s]]     loss: -2.3045454       [[val 0.7121s]]     loss: -2.26819124      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     7500]]     [[train 2.9486s]]     loss: -2.30268035      [[val 0.7164s]]     loss: -2.2678357       
[[step     7520]]     [[train 2.9391s]]     loss: -2.30802785      [[val 0.7112s]]     loss: -2.270206        
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     7540]]     [[train 2.926s]]     loss: -2.30268387      [[val 0.712s]]     loss: -2.26918925      
[[step     7560]]     [[train 2.8807s]]     loss: -2.29863927      [[val 0.7099s]]     loss: -2.26794658      
[[step     7580]]     [[train 2.889s]]     loss: -2.29584219      [[val 0.7091s]]     loss: -2.26555687      
[[step     7600]]     [[train 2.9004s]]     loss: -2.29723863      [[val 0.7107s]]     loss: -2.26235673      
[[step     7620]]     [[train 2.9155s]]     loss: -2.29298653      [[val 0.7158s]]     loss: -2.26111797      
[[step     7640]]     [[train 2.8975s]]     loss: -2.30080604      [[val 0.7226s]]     loss: -2.25969836      
[[step     7660]]     [[train 2.927s]]     loss: -2.30205549      [[val 0.7217s]]     loss: -2.26296435      
[[step     7680]]     [[train 2.9122s]]     loss: -2.30694987      [[val 0.7257s]]     loss: -2.26619755      
[[step     7700]]     [[train 2.9066s]]     loss: -2.31014531      [[val 0.724s]]     loss: -2.27218065      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     7720]]     [[train 2.9163s]]     loss: -2.31108679      [[val 0.727s]]     loss: -2.26660005      
[[step     7740]]     [[train 2.915s]]     loss: -2.30346146      [[val 0.7209s]]     loss: -2.27002328      
[[step     7760]]     [[train 2.9098s]]     loss: -2.30525867      [[val 0.7182s]]     loss: -2.26816085      
[[step     7780]]     [[train 2.9315s]]     loss: -2.30274833      [[val 0.711s]]     loss: -2.2659525       
[[step     7800]]     [[train 2.9442s]]     loss: -2.30388059      [[val 0.7158s]]     loss: -2.26392973      
[[step     7820]]     [[train 2.9298s]]     loss: -2.30726375      [[val 0.7047s]]     loss: -2.27381631      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     7840]]     [[train 2.9338s]]     loss: -2.31169109      [[val 0.7049s]]     loss: -2.2731436       
[[step     7860]]     [[train 2.9346s]]     loss: -2.3120885       [[val 0.706s]]     loss: -2.27583383      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     7880]]     [[train 2.9454s]]     loss: -2.30721419      [[val 0.71s]]     loss: -2.27759225      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     7900]]     [[train 2.9309s]]     loss: -2.30687079      [[val 0.7074s]]     loss: -2.27718323      
[[step     7920]]     [[train 2.9467s]]     loss: -2.30921223      [[val 0.7123s]]     loss: -2.27350642      
[[step     7940]]     [[train 2.9709s]]     loss: -2.30737368      [[val 0.7128s]]     loss: -2.27024294      
[[step     7960]]     [[train 2.9659s]]     loss: -2.31060814      [[val 0.7117s]]     loss: -2.27026538      
[[step     7980]]     [[train 2.94s]]     loss: -2.31908253      [[val 0.7124s]]     loss: -2.27166343      
[[step     8000]]     [[train 2.9481s]]     loss: -2.32047618      [[val 0.7173s]]     loss: -2.27549204      
[[step     8020]]     [[train 2.9315s]]     loss: -2.32136625      [[val 0.7222s]]     loss: -2.27507827      
[[step     8040]]     [[train 2.9081s]]     loss: -2.32792255      [[val 0.7264s]]     loss: -2.2797566       
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     8060]]     [[train 2.9289s]]     loss: -2.32234014      [[val 0.7268s]]     loss: -2.2772973       
[[step     8080]]     [[train 2.9257s]]     loss: -2.3231901       [[val 0.7246s]]     loss: -2.27964122      
[[step     8100]]     [[train 2.9187s]]     loss: -2.32789886      [[val 0.7251s]]     loss: -2.27665672      
[[step     8120]]     [[train 2.9155s]]     loss: -2.32466511      [[val 0.7167s]]     loss: -2.27774168      
[[step     8140]]     [[train 2.9361s]]     loss: -2.32148022      [[val 0.7197s]]     loss: -2.27813057      
[[step     8160]]     [[train 2.9097s]]     loss: -2.32529265      [[val 0.7261s]]     loss: -2.28128292      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     8180]]     [[train 2.9082s]]     loss: -2.32495668      [[val 0.7244s]]     loss: -2.28144156      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     8200]]     [[train 2.9164s]]     loss: -2.31894859      [[val 0.7173s]]     loss: -2.28364287      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     8220]]     [[train 2.9119s]]     loss: -2.3199569       [[val 0.7203s]]     loss: -2.28643614      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     8240]]     [[train 2.899s]]     loss: -2.31781733      [[val 0.7141s]]     loss: -2.28607762      
[[step     8260]]     [[train 2.8722s]]     loss: -2.32118309      [[val 0.7075s]]     loss: -2.28537945      
[[step     8280]]     [[train 2.8657s]]     loss: -2.3280432       [[val 0.7065s]]     loss: -2.28452327      
[[step     8300]]     [[train 2.8569s]]     loss: -2.32972465      [[val 0.7056s]]     loss: -2.28418172      
[[step     8320]]     [[train 2.8652s]]     loss: -2.33376272      [[val 0.7073s]]     loss: -2.28206814      
[[step     8340]]     [[train 2.8806s]]     loss: -2.33737939      [[val 0.7085s]]     loss: -2.28231124      
[[step     8360]]     [[train 2.9256s]]     loss: -2.33710652      [[val 0.7161s]]     loss: -2.27906791      
[[step     8380]]     [[train 2.9248s]]     loss: -2.33111204      [[val 0.719s]]     loss: -2.28120436      
[[step     8400]]     [[train 2.9266s]]     loss: -2.33320115      [[val 0.7222s]]     loss: -2.28288613      
[[step     8420]]     [[train 2.9375s]]     loss: -2.32809201      [[val 0.7164s]]     loss: -2.28289447      
[[step     8440]]     [[train 2.9178s]]     loss: -2.32659417      [[val 0.7201s]]     loss: -2.28477898      
[[step     8460]]     [[train 2.8834s]]     loss: -2.31954927      [[val 0.7184s]]     loss: -2.28378695      
[[step     8480]]     [[train 2.8897s]]     loss: -2.3229276       [[val 0.719s]]     loss: -2.28284936      
[[step     8500]]     [[train 2.9001s]]     loss: -2.32585281      [[val 0.7215s]]     loss: -2.28406517      
[[step     8520]]     [[train 2.8845s]]     loss: -2.32873128      [[val 0.729s]]     loss: -2.2841512       
[[step     8540]]     [[train 2.8992s]]     loss: -2.32468833      [[val 0.7227s]]     loss: -2.28389791      
[[step     8560]]     [[train 2.9422s]]     loss: -2.32684669      [[val 0.7251s]]     loss: -2.2895197       
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     8580]]     [[train 2.9427s]]     loss: -2.32346515      [[val 0.7269s]]     loss: -2.28753317      
[[step     8600]]     [[train 2.9214s]]     loss: -2.32563165      [[val 0.7229s]]     loss: -2.28649685      
[[step     8620]]     [[train 2.9331s]]     loss: -2.33215613      [[val 0.7239s]]     loss: -2.2879815       
[[step     8640]]     [[train 2.9233s]]     loss: -2.3375241       [[val 0.7227s]]     loss: -2.28997695      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     8660]]     [[train 2.9207s]]     loss: -2.33944633      [[val 0.721s]]     loss: -2.28781633      
[[step     8680]]     [[train 2.9476s]]     loss: -2.3409947       [[val 0.7235s]]     loss: -2.29106526      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     8700]]     [[train 2.9484s]]     loss: -2.33736321      [[val 0.7224s]]     loss: -2.28967867      
[[step     8720]]     [[train 2.9473s]]     loss: -2.33529786      [[val 0.7182s]]     loss: -2.29023469      
[[step     8740]]     [[train 3.0131s]]     loss: -2.33515105      [[val 0.7528s]]     loss: -2.28761365      
[[step     8760]]     [[train 3.0758s]]     loss: -2.3407269       [[val 0.7807s]]     loss: -2.29086824      
[[step     8780]]     [[train 3.1591s]]     loss: -2.34138261      [[val 0.8076s]]     loss: -2.29098531      
[[step     8800]]     [[train 3.2739s]]     loss: -2.34625832      [[val 0.8439s]]     loss: -2.29151352      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     8820]]     [[train 3.3221s]]     loss: -2.34334356      [[val 0.8594s]]     loss: -2.29353706      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     8840]]     [[train 3.2367s]]     loss: -2.34510745      [[val 0.8256s]]     loss: -2.29332799      
[[step     8860]]     [[train 3.1624s]]     loss: -2.33778735      [[val 0.7992s]]     loss: -2.29123536      
[[step     8880]]     [[train 3.061s]]     loss: -2.33713042      [[val 0.7723s]]     loss: -2.29166898      
[[step     8900]]     [[train 2.9642s]]     loss: -2.33139222      [[val 0.7422s]]     loss: -2.29174209      
[[step     8920]]     [[train 2.927s]]     loss: -2.33580709      [[val 0.7321s]]     loss: -2.28802173      
[[step     8940]]     [[train 2.9534s]]     loss: -2.33675004      [[val 0.7373s]]     loss: -2.29147542      
[[step     8960]]     [[train 2.9343s]]     loss: -2.34568714      [[val 0.7308s]]     loss: -2.2904919       
[[step     8980]]     [[train 2.9199s]]     loss: -2.34523495      [[val 0.7285s]]     loss: -2.28973468      
[[step     9000]]     [[train 2.921s]]     loss: -2.345764        [[val 0.7287s]]     loss: -2.29169066      
[[step     9020]]     [[train 2.9191s]]     loss: -2.34751388      [[val 0.7264s]]     loss: -2.29522246      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     9040]]     [[train 2.9092s]]     loss: -2.34668222      [[val 0.725s]]     loss: -2.29139976      
[[step     9060]]     [[train 2.9152s]]     loss: -2.34492766      [[val 0.7264s]]     loss: -2.29739152      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     9080]]     [[train 2.8968s]]     loss: -2.34327461      [[val 0.7269s]]     loss: -2.29711033      
[[step     9100]]     [[train 2.9095s]]     loss: -2.34600466      [[val 0.7307s]]     loss: -2.29664418      
[[step     9120]]     [[train 2.92s]]     loss: -2.34623498      [[val 0.7304s]]     loss: -2.29571721      
[[step     9140]]     [[train 2.9271s]]     loss: -2.3476648       [[val 0.7308s]]     loss: -2.29700918      
[[step     9160]]     [[train 2.9379s]]     loss: -2.34431733      [[val 0.7355s]]     loss: -2.29389412      
[[step     9180]]     [[train 2.9505s]]     loss: -2.34911902      [[val 0.736s]]     loss: -2.29727922      
[[step     9200]]     [[train 2.9236s]]     loss: -2.35240504      [[val 0.7303s]]     loss: -2.29625773      
[[step     9220]]     [[train 2.9261s]]     loss: -2.35326782      [[val 0.7309s]]     loss: -2.29697625      
[[step     9240]]     [[train 2.91s]]     loss: -2.35334285      [[val 0.7323s]]     loss: -2.30001635      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     9260]]     [[train 2.9133s]]     loss: -2.3556109       [[val 0.7289s]]     loss: -2.29802618      
[[step     9280]]     [[train 2.9416s]]     loss: -2.35791208      [[val 0.7283s]]     loss: -2.29609899      
[[step     9300]]     [[train 2.9427s]]     loss: -2.35877753      [[val 0.7313s]]     loss: -2.29706204      
[[step     9320]]     [[train 2.9207s]]     loss: -2.35465786      [[val 0.7305s]]     loss: -2.29797667      
[[step     9340]]     [[train 2.9117s]]     loss: -2.35956382      [[val 0.7236s]]     loss: -2.29568899      
[[step     9360]]     [[train 2.922s]]     loss: -2.36305024      [[val 0.7222s]]     loss: -2.30032444      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     9380]]     [[train 2.8946s]]     loss: -2.36648044      [[val 0.7207s]]     loss: -2.29987867      
[[step     9400]]     [[train 2.9567s]]     loss: -2.36384529      [[val 0.7359s]]     loss: -2.29972944      
[[step     9420]]     [[train 3.0433s]]     loss: -2.36272867      [[val 0.7709s]]     loss: -2.30231311      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     9440]]     [[train 3.1596s]]     loss: -2.35754432      [[val 0.8109s]]     loss: -2.30291763      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     9460]]     [[train 3.2432s]]     loss: -2.35210427      [[val 0.8497s]]     loss: -2.30186656      
[[step     9480]]     [[train 3.2595s]]     loss: -2.34761102      [[val 0.8573s]]     loss: -2.30230796      
[[step     9500]]     [[train 3.1929s]]     loss: -2.34829543      [[val 0.8426s]]     loss: -2.30527753      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     9520]]     [[train 3.0965s]]     loss: -2.35572931      [[val 0.8067s]]     loss: -2.30368021      
[[step     9540]]     [[train 2.9857s]]     loss: -2.356872        [[val 0.7718s]]     loss: -2.30473832      
[[step     9560]]     [[train 2.8883s]]     loss: -2.36276026      [[val 0.7351s]]     loss: -2.29988718      
[[step     9580]]     [[train 2.8849s]]     loss: -2.36621641      [[val 0.7252s]]     loss: -2.30360857      
[[step     9600]]     [[train 2.9925s]]     loss: -2.36844649      [[val 0.7507s]]     loss: -2.30039245      
[[step     9620]]     [[train 3.1232s]]     loss: -2.36592381      [[val 0.78s]]     loss: -2.29907605      
[[step     9640]]     [[train 3.2492s]]     loss: -2.3731911       [[val 0.8156s]]     loss: -2.29976958      
[[step     9660]]     [[train 3.3873s]]     loss: -2.3742884       [[val 0.8419s]]     loss: -2.3040031       
[[step     9680]]     [[train 3.5212s]]     loss: -2.37466423      [[val 0.875s]]     loss: -2.30136871      
[[step     9700]]     [[train 3.5524s]]     loss: -2.36802974      [[val 0.8759s]]     loss: -2.30128773      
[[step     9720]]     [[train 3.5918s]]     loss: -2.3646849       [[val 0.8701s]]     loss: -2.30360045      
[[step     9740]]     [[train 3.587s]]     loss: -2.3586188       [[val 0.8667s]]     loss: -2.29970319      
[[step     9760]]     [[train 3.5558s]]     loss: -2.35744784      [[val 0.8715s]]     loss: -2.30206013      
[[step     9780]]     [[train 3.5393s]]     loss: -2.35462183      [[val 0.87s]]     loss: -2.30286215      
[[step     9800]]     [[train 3.533s]]     loss: -2.35867735      [[val 0.8769s]]     loss: -2.30421607      
[[step     9820]]     [[train  3.5s]]     loss: -2.35855383      [[val 0.884s]]     loss: -2.30114862      
[[step     9840]]     [[train 3.5338s]]     loss: -2.35833323      [[val 0.8878s]]     loss: -2.30544992      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     9860]]     [[train 3.5405s]]     loss: -2.36205009      [[val 0.8862s]]     loss: -2.30606078      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     9880]]     [[train 3.5299s]]     loss: -2.36423721      [[val 0.8956s]]     loss: -2.30653153      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     9900]]     [[train 3.5268s]]     loss: -2.37109257      [[val 0.8823s]]     loss: -2.3085739       
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     9920]]     [[train 3.5306s]]     loss: -2.37769998      [[val 0.88s]]     loss: -2.30914549      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step     9940]]     [[train 3.5326s]]     loss: -2.37472222      [[val 0.8734s]]     loss: -2.30661066      
[[step     9960]]     [[train 3.5371s]]     loss: -2.37420532      [[val 0.8715s]]     loss: -2.30528681      
[[step     9980]]     [[train 3.5848s]]     loss: -2.37484743      [[val 0.8642s]]     loss: -2.30476893      
[[step    10000]]     [[train 3.5822s]]     loss: -2.3742522       [[val 0.8694s]]     loss: -2.30643162      
[[step    10020]]     [[train 3.6193s]]     loss: -2.37011017      [[val 0.8727s]]     loss: -2.3061033       
[[step    10040]]     [[train 3.6068s]]     loss: -2.3705966       [[val 0.8651s]]     loss: -2.30587214      
[[step    10060]]     [[train 3.5984s]]     loss: -2.37134228      [[val 0.8673s]]     loss: -2.30526553      
[[step    10080]]     [[train 3.5678s]]     loss: -2.37351297      [[val 0.8616s]]     loss: -2.305873        
[[step    10100]]     [[train 3.5806s]]     loss: -2.36936047      [[val 0.8549s]]     loss: -2.30056302      
[[step    10120]]     [[train 3.5287s]]     loss: -2.36743165      [[val 0.8581s]]     loss: -2.30511393      
[[step    10140]]     [[train 3.502s]]     loss: -2.3729563       [[val 0.8624s]]     loss: -2.30646895      
[[step    10160]]     [[train 3.4977s]]     loss: -2.3715118       [[val 0.8632s]]     loss: -2.3063677       
[[step    10180]]     [[train 3.5116s]]     loss: -2.37199957      [[val 0.8677s]]     loss: -2.30672262      
[[step    10200]]     [[train 3.4667s]]     loss: -2.37507219      [[val 0.8781s]]     loss: -2.30877793      
[[step    10220]]     [[train 3.4564s]]     loss: -2.37750575      [[val 0.8734s]]     loss: -2.30309178      
[[step    10240]]     [[train 3.4537s]]     loss: -2.38303382      [[val 0.8728s]]     loss: -2.30329626      
[[step    10260]]     [[train 3.4697s]]     loss: -2.38437585      [[val 0.8757s]]     loss: -2.30767832      
[[step    10280]]     [[train 3.4531s]]     loss: -2.38427793      [[val 0.8807s]]     loss: -2.3054563       
[[step    10300]]     [[train 3.5103s]]     loss: -2.38718639      [[val 0.8738s]]     loss: -2.30797591      
[[step    10320]]     [[train 3.4969s]]     loss: -2.39347928      [[val 0.8732s]]     loss: -2.31346455      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step    10340]]     [[train 3.5167s]]     loss: -2.38730984      [[val 0.8723s]]     loss: -2.31564094      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step    10360]]     [[train 3.5201s]]     loss: -2.38692085      [[val 0.8603s]]     loss: -2.31581489      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step    10380]]     [[train 3.5035s]]     loss: -2.38307382      [[val 0.8565s]]     loss: -2.31409466      
[[step    10400]]     [[train 3.452s]]     loss: -2.38362339      [[val 0.8531s]]     loss: -2.31439596      
[[step    10420]]     [[train 3.4858s]]     loss: -2.37842544      [[val 0.8598s]]     loss: -2.31368588      
[[step    10440]]     [[train 3.4436s]]     loss: -2.37851387      [[val 0.8677s]]     loss: -2.31408524      
[[step    10460]]     [[train 3.4503s]]     loss: -2.37122926      [[val 0.8784s]]     loss: -2.31274535      
[[step    10480]]     [[train 3.4528s]]     loss: -2.37989681      [[val 0.8747s]]     loss: -2.31523056      
[[step    10500]]     [[train 3.4466s]]     loss: -2.37821485      [[val 0.8794s]]     loss: -2.31342427      
[[step    10520]]     [[train 3.4061s]]     loss: -2.38146043      [[val 0.867s]]     loss: -2.31096281      
[[step    10540]]     [[train 3.4368s]]     loss: -2.38403083      [[val 0.8695s]]     loss: -2.31158174      
[[step    10560]]     [[train 3.4378s]]     loss: -2.38983796      [[val 0.8687s]]     loss: -2.30944435      
[[step    10580]]     [[train 3.4005s]]     loss: -2.38593421      [[val 0.8612s]]     loss: -2.3098324       
[[step    10600]]     [[train 3.4513s]]     loss: -2.3864869       [[val 0.8568s]]     loss: -2.30982407      
[[step    10620]]     [[train 3.4826s]]     loss: -2.38490333      [[val 0.8589s]]     loss: -2.31294115      
[[step    10640]]     [[train 3.4732s]]     loss: -2.38327189      [[val 0.8528s]]     loss: -2.31240241      
[[step    10660]]     [[train 3.4553s]]     loss: -2.3891706       [[val 0.8638s]]     loss: -2.31329041      
[[step    10680]]     [[train 3.5874s]]     loss: -2.38312584      [[val 0.897s]]     loss: -2.31246218      
[[step    10700]]     [[train 3.7043s]]     loss: -2.38245125      [[val 0.9333s]]     loss: -2.3140108       
[[step    10720]]     [[train 3.806s]]     loss: -2.38853618      [[val 0.9677s]]     loss: -2.31330527      
[[step    10740]]     [[train 3.9104s]]     loss: -2.3927241       [[val 1.0029s]]     loss: -2.31144803      
[[step    10760]]     [[train 3.9692s]]     loss: -2.38509691      [[val 1.0138s]]     loss: -2.31311783      
[[step    10780]]     [[train 3.8974s]]     loss: -2.38763211      [[val 0.9992s]]     loss: -2.31458643      
[[step    10800]]     [[train 3.7793s]]     loss: -2.38896487      [[val 0.9773s]]     loss: -2.31366121      
[[step    10820]]     [[train 3.7208s]]     loss: -2.38763993      [[val 0.9558s]]     loss: -2.31163208      
[[step    10840]]     [[train 3.6823s]]     loss: -2.38980405      [[val 0.9325s]]     loss: -2.31175457      
[[step    10860]]     [[train 3.6572s]]     loss: -2.39703024      [[val 0.9089s]]     loss: -2.3083843       
[[step    10880]]     [[train 3.6354s]]     loss: -2.40303617      [[val 0.895s]]     loss: -2.30812668      
[[step    10900]]     [[train 3.6293s]]     loss: -2.3991324       [[val 0.8922s]]     loss: -2.31048525      
[[step    10920]]     [[train 3.5888s]]     loss: -2.39767453      [[val 0.8917s]]     loss: -2.31311456      
[[step    10940]]     [[train 3.5683s]]     loss: -2.39204219      [[val 0.8842s]]     loss: -2.3109885       
[[step    10960]]     [[train 3.5325s]]     loss: -2.39096018      [[val 0.8918s]]     loss: -2.31379375      
[[step    10980]]     [[train 3.5598s]]     loss: -2.38834697      [[val 0.8936s]]     loss: -2.31237162      
[[step    11000]]     [[train 3.553s]]     loss: -2.38944279      [[val 0.8904s]]     loss: -2.30929511      
[[step    11020]]     [[train 3.581s]]     loss: -2.38917192      [[val 0.8794s]]     loss: -2.31170039      
[[step    11040]]     [[train 3.5838s]]     loss: -2.38473831      [[val 0.8795s]]     loss: -2.31174987      
[[step    11060]]     [[train 3.5956s]]     loss: -2.38146246      [[val 0.8713s]]     loss: -2.31122582      
[[step    11080]]     [[train 3.5723s]]     loss: -2.38415685      [[val 0.875s]]     loss: -2.31507797      
[[step    11100]]     [[train 3.5574s]]     loss: -2.38895087      [[val 0.8811s]]     loss: -2.31907037      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step    11120]]     [[train 3.5474s]]     loss: -2.38918434      [[val 0.8803s]]     loss: -2.31514066      
[[step    11140]]     [[train 3.5171s]]     loss: -2.39933023      [[val 0.8761s]]     loss: -2.31869195      
[[step    11160]]     [[train 3.5074s]]     loss: -2.40231942      [[val 0.8796s]]     loss: -2.32002553      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step    11180]]     [[train 3.5314s]]     loss: -2.40344205      [[val 0.873s]]     loss: -2.31314568      
[[step    11200]]     [[train 3.5064s]]     loss: -2.4063515       [[val 0.8595s]]     loss: -2.31235134      
[[step    11220]]     [[train 3.4782s]]     loss: -2.41077265      [[val 0.8567s]]     loss: -2.31339361      
[[step    11240]]     [[train 3.4492s]]     loss: -2.40803757      [[val 0.8546s]]     loss: -2.31355513      
[[step    11260]]     [[train 3.3199s]]     loss: -2.40734895      [[val 0.8197s]]     loss: -2.31365864      
[[step    11280]]     [[train 3.1801s]]     loss: -2.40695802      [[val 0.7959s]]     loss: -2.31785032      
[[step    11300]]     [[train 3.0888s]]     loss: -2.40536649      [[val 0.7709s]]     loss: -2.31637318      
[[step    11320]]     [[train 2.9934s]]     loss: -2.40402556      [[val 0.75s]]     loss: -2.31884516      
[[step    11340]]     [[train 2.9075s]]     loss: -2.40659151      [[val 0.7217s]]     loss: -2.32198839      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step    11360]]     [[train 2.9132s]]     loss: -2.4057407       [[val 0.7233s]]     loss: -2.31840784      
[[step    11380]]     [[train 2.9177s]]     loss: -2.39970833      [[val 0.72s]]     loss: -2.32202209      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step    11400]]     [[train 2.9455s]]     loss: -2.39433567      [[val 0.7217s]]     loss: -2.3200492       
[[step    11420]]     [[train 2.9398s]]     loss: -2.39373041      [[val 0.716s]]     loss: -2.3187526       
[[step    11440]]     [[train 2.9265s]]     loss: -2.39686344      [[val 0.7149s]]     loss: -2.31667305      
[[step    11460]]     [[train 2.941s]]     loss: -2.40182721      [[val 0.7143s]]     loss: -2.32051559      
[[step    11480]]     [[train 2.9412s]]     loss: -2.40171866      [[val 0.7206s]]     loss: -2.31226012      
[[step    11500]]     [[train 2.9233s]]     loss: -2.41108633      [[val 0.7233s]]     loss: -2.31188816      
[[step    11520]]     [[train 2.8956s]]     loss: -2.40898408      [[val 0.7292s]]     loss: -2.31198137      
[[step    11540]]     [[train 2.8745s]]     loss: -2.40785784      [[val 0.7273s]]     loss: -2.31023356      
[[step    11560]]     [[train 2.8612s]]     loss: -2.41113299      [[val 0.7265s]]     loss: -2.30856694      
[[step    11580]]     [[train 2.8563s]]     loss: -2.41614522      [[val 0.7288s]]     loss: -2.31562087      
[[step    11600]]     [[train 2.8729s]]     loss: -2.41201182      [[val 0.7269s]]     loss: -2.3174363       
[[step    11620]]     [[train 2.8782s]]     loss: -2.41436055      [[val 0.7214s]]     loss: -2.3184278       
[[step    11640]]     [[train 2.9487s]]     loss: -2.40957333      [[val 0.7348s]]     loss: -2.31870204      
[[step    11660]]     [[train 3.0281s]]     loss: -2.40510015      [[val 0.7613s]]     loss: -2.31975302      
[[step    11680]]     [[train 3.1619s]]     loss: -2.40660542      [[val 0.7791s]]     loss: -2.31960776      
[[step    11700]]     [[train 3.2613s]]     loss: -2.40463817      [[val 0.8024s]]     loss: -2.32215931      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step    11720]]     [[train 3.3643s]]     loss: -2.40439424      [[val 0.8381s]]     loss: -2.32272731      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step    11740]]     [[train 3.4259s]]     loss: -2.40596273      [[val 0.8538s]]     loss: -2.3231038       
saving model to checkpoints/1564954698_ordertests_mean/model
[[step    11760]]     [[train 3.4192s]]     loss: -2.40357391      [[val 0.8522s]]     loss: -2.31944452      
[[step    11780]]     [[train 3.3671s]]     loss: -2.40903455      [[val 0.8504s]]     loss: -2.3193871       
[[step    11800]]     [[train 3.3038s]]     loss: -2.41593232      [[val 0.8524s]]     loss: -2.31968134      
[[step    11820]]     [[train 3.288s]]     loss: -2.41195328      [[val 0.8423s]]     loss: -2.31625492      
[[step    11840]]     [[train 3.326s]]     loss: -2.41894517      [[val 0.8575s]]     loss: -2.31641648      
[[step    11860]]     [[train 3.4377s]]     loss: -2.42670578      [[val 0.8846s]]     loss: -2.32393168      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step    11880]]     [[train 3.5237s]]     loss: -2.42006561      [[val 0.904s]]     loss: -2.32266239      
[[step    11900]]     [[train 3.613s]]     loss: -2.42087993      [[val 0.9196s]]     loss: -2.3203566       
[[step    11920]]     [[train 3.6366s]]     loss: -2.42932142      [[val 0.931s]]     loss: -2.32234484      
[[step    11940]]     [[train 3.6651s]]     loss: -2.42215382      [[val 0.9342s]]     loss: -2.32378576      
[[step    11960]]     [[train 3.623s]]     loss: -2.41673559      [[val 0.9216s]]     loss: -2.32051076      
[[step    11980]]     [[train 3.6523s]]     loss: -2.41599473      [[val 0.9365s]]     loss: -2.31970361      
[[step    12000]]     [[train 3.6435s]]     loss: -2.40797001      [[val 0.9277s]]     loss: -2.32097536      
[[step    12020]]     [[train 3.7011s]]     loss: -2.40132274      [[val 0.9268s]]     loss: -2.31898041      
[[step    12040]]     [[train 3.6991s]]     loss: -2.40604526      [[val 0.9294s]]     loss: -2.32146177      
[[step    12060]]     [[train 3.7766s]]     loss: -2.40726946      [[val 0.9394s]]     loss: -2.31969087      
[[step    12080]]     [[train 3.7507s]]     loss: -2.41876284      [[val 0.9259s]]     loss: -2.32129486      
[[step    12100]]     [[train 3.7484s]]     loss: -2.42956817      [[val 0.9264s]]     loss: -2.3202619       
[[step    12120]]     [[train 3.8194s]]     loss: -2.434557        [[val 0.9531s]]     loss: -2.31983385      
[[step    12140]]     [[train 3.8031s]]     loss: -2.43241568      [[val 0.951s]]     loss: -2.31360071      
[[step    12160]]     [[train 3.7335s]]     loss: -2.43234212      [[val 0.9418s]]     loss: -2.3138138       
[[step    12180]]     [[train 3.6664s]]     loss: -2.42323567      [[val 0.9312s]]     loss: -2.31356488      
[[step    12200]]     [[train 3.6631s]]     loss: -2.4134745       [[val 0.9255s]]     loss: -2.3134657       
[[step    12220]]     [[train 3.54s]]     loss: -2.41375641      [[val 0.8933s]]     loss: -2.31322232      
[[step    12240]]     [[train 3.5183s]]     loss: -2.41636106      [[val 0.8793s]]     loss: -2.31748652      
[[step    12260]]     [[train 3.4788s]]     loss: -2.41479124      [[val 0.8637s]]     loss: -2.31590505      
[[step    12280]]     [[train 3.4463s]]     loss: -2.41367603      [[val 0.8589s]]     loss: -2.31301746      
[[step    12300]]     [[train 3.42s]]     loss: -2.40548143      [[val 0.8508s]]     loss: -2.30994395      
[[step    12320]]     [[train 3.4061s]]     loss: -2.40494851      [[val 0.8497s]]     loss: -2.31003628      
[[step    12340]]     [[train 3.3779s]]     loss: -2.40442199      [[val 0.845s]]     loss: -2.30779112      
[[step    12360]]     [[train 3.3337s]]     loss: -2.40727242      [[val 0.8444s]]     loss: -2.31414927      
[[step    12380]]     [[train 3.3506s]]     loss: -2.41066952      [[val 0.8369s]]     loss: -2.31432928      
[[step    12400]]     [[train 3.3126s]]     loss: -2.42512171      [[val 0.8498s]]     loss: -2.31700297      
[[step    12420]]     [[train 3.2915s]]     loss: -2.42518776      [[val 0.8511s]]     loss: -2.31525676      
[[step    12440]]     [[train 3.2614s]]     loss: -2.43106107      [[val 0.8519s]]     loss: -2.31945048      
[[step    12460]]     [[train 3.2942s]]     loss: -2.43016469      [[val 0.8462s]]     loss: -2.31536686      
[[step    12480]]     [[train 3.2618s]]     loss: -2.43281853      [[val 0.854s]]     loss: -2.31713677      
[[step    12500]]     [[train 3.2925s]]     loss: -2.43442271      [[val 0.8525s]]     loss: -2.31847763      
[[step    12520]]     [[train 3.2824s]]     loss: -2.43705195      [[val 0.8501s]]     loss: -2.32089671      
[[step    12540]]     [[train 3.2823s]]     loss: -2.42546273      [[val 0.8483s]]     loss: -2.31917913      
[[step    12560]]     [[train 3.271s]]     loss: -2.4258685       [[val 0.8579s]]     loss: -2.31803526      
[[step    12580]]     [[train 3.2922s]]     loss: -2.42602757      [[val 0.8499s]]     loss: -2.32150555      
[[step    12600]]     [[train 3.2872s]]     loss: -2.42391536      [[val 0.843s]]     loss: -2.32110977      
[[step    12620]]     [[train 3.2962s]]     loss: -2.4220064       [[val 0.8494s]]     loss: -2.32167593      
[[step    12640]]     [[train 3.3166s]]     loss: -2.42567549      [[val 0.8505s]]     loss: -2.31886451      
[[step    12660]]     [[train 3.3138s]]     loss: -2.42588804      [[val 0.8484s]]     loss: -2.32131561      
[[step    12680]]     [[train 3.34s]]     loss: -2.42628286      [[val 0.853s]]     loss: -2.32275336      
[[step    12700]]     [[train 3.3467s]]     loss: -2.42656324      [[val 0.8593s]]     loss: -2.32114493      
[[step    12720]]     [[train 3.3644s]]     loss: -2.43344182      [[val 0.8598s]]     loss: -2.32180064      
[[step    12740]]     [[train 3.3351s]]     loss: -2.44035362      [[val 0.8603s]]     loss: -2.32240913      
[[step    12760]]     [[train 3.3031s]]     loss: -2.4485774       [[val 0.8605s]]     loss: -2.32160578      
[[step    12780]]     [[train 3.2995s]]     loss: -2.44354566      [[val 0.8647s]]     loss: -2.31676718      
[[step    12800]]     [[train 3.2784s]]     loss: -2.44143035      [[val 0.8545s]]     loss: -2.3177143       
[[step    12820]]     [[train 3.2709s]]     loss: -2.43627565      [[val 0.8472s]]     loss: -2.31872733      
[[step    12840]]     [[train 3.2565s]]     loss: -2.43594862      [[val 0.8486s]]     loss: -2.31808086      
[[step    12860]]     [[train 3.2748s]]     loss: -2.43066846      [[val 0.8482s]]     loss: -2.31669528      
[[step    12880]]     [[train 3.2561s]]     loss: -2.43204982      [[val 0.8454s]]     loss: -2.31759044      
[[step    12900]]     [[train 3.2216s]]     loss: -2.43272795      [[val 0.8475s]]     loss: -2.31899632      
[[step    12920]]     [[train 3.2036s]]     loss: -2.4289406       [[val 0.8495s]]     loss: -2.31713244      
[[step    12940]]     [[train 3.2013s]]     loss: -2.42097808      [[val 0.8455s]]     loss: -2.31906066      
[[step    12960]]     [[train 3.1963s]]     loss: -2.4227397       [[val 0.8455s]]     loss: -2.32120751      
[[step    12980]]     [[train 3.1842s]]     loss: -2.42809401      [[val 0.844s]]     loss: -2.3223748       
[[step    13000]]     [[train 3.2219s]]     loss: -2.42990004      [[val 0.8556s]]     loss: -2.32086835      
[[step    13020]]     [[train 3.2441s]]     loss: -2.4351677       [[val 0.8497s]]     loss: -2.31735106      
[[step    13040]]     [[train 3.2597s]]     loss: -2.44771501      [[val 0.8456s]]     loss: -2.31977881      
[[step    13060]]     [[train 3.2681s]]     loss: -2.45203015      [[val 0.8454s]]     loss: -2.31586499      
[[step    13080]]     [[train 3.2509s]]     loss: -2.4573886       [[val 0.8459s]]     loss: -2.3159299       
[[step    13100]]     [[train 3.2469s]]     loss: -2.4564442       [[val 0.8388s]]     loss: -2.31571589      
[[step    13120]]     [[train 3.234s]]     loss: -2.45423706      [[val 0.8438s]]     loss: -2.32038105      
[[step    13140]]     [[train 3.2081s]]     loss: -2.44799571      [[val 0.839s]]     loss: -2.31549299      
[[step    13160]]     [[train 3.201s]]     loss: -2.44553936      [[val 0.8417s]]     loss: -2.31962176      
[[step    13180]]     [[train 3.1825s]]     loss: -2.43830274      [[val 0.8408s]]     loss: -2.32035281      
[[step    13200]]     [[train 3.1256s]]     loss: -2.43681201      [[val 0.8377s]]     loss: -2.32308264      
[[step    13220]]     [[train 3.1562s]]     loss: -2.43679606      [[val 0.8399s]]     loss: -2.32274843      
[[step    13240]]     [[train 3.1969s]]     loss: -2.43582616      [[val 0.8494s]]     loss: -2.32614079      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step    13260]]     [[train 3.2096s]]     loss: -2.43269805      [[val 0.8465s]]     loss: -2.32618011      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step    13280]]     [[train 3.2224s]]     loss: -2.4324096       [[val 0.8478s]]     loss: -2.32537729      
[[step    13300]]     [[train 3.2829s]]     loss: -2.42901657      [[val 0.848s]]     loss: -2.32474603      
[[step    13320]]     [[train 3.2394s]]     loss: -2.43191994      [[val 0.8434s]]     loss: -2.32490206      
[[step    13340]]     [[train 3.2239s]]     loss: -2.43641896      [[val 0.8519s]]     loss: -2.32068121      
[[step    13360]]     [[train 3.2114s]]     loss: -2.44400558      [[val 0.85s]]     loss: -2.3202313       
[[step    13380]]     [[train 3.2316s]]     loss: -2.44319722      [[val 0.8566s]]     loss: -2.3122988       
[[step    13400]]     [[train 3.2509s]]     loss: -2.45132294      [[val 0.8519s]]     loss: -2.3119808       
[[step    13420]]     [[train 3.2627s]]     loss: -2.45201338      [[val 0.8559s]]     loss: -2.31176707      
[[step    13440]]     [[train 3.2604s]]     loss: -2.45005891      [[val 0.8531s]]     loss: -2.3147175       
[[step    13460]]     [[train 3.2598s]]     loss: -2.4444543       [[val 0.8589s]]     loss: -2.31303677      
[[step    13480]]     [[train 3.2586s]]     loss: -2.44677504      [[val 0.8475s]]     loss: -2.31690574      
[[step    13500]]     [[train 3.207s]]     loss: -2.44333241      [[val 0.8531s]]     loss: -2.31889646      
[[step    13520]]     [[train 3.2098s]]     loss: -2.44386912      [[val 0.8464s]]     loss: -2.31962303      
[[step    13540]]     [[train 3.1973s]]     loss: -2.44362164      [[val 0.8384s]]     loss: -2.32003179      
[[step    13560]]     [[train 3.1934s]]     loss: -2.44634334      [[val 0.8332s]]     loss: -2.32181137      
[[step    13580]]     [[train 3.2001s]]     loss: -2.44697747      [[val 0.8407s]]     loss: -2.32544321      
[[step    13600]]     [[train 3.1962s]]     loss: -2.45220641      [[val 0.8382s]]     loss: -2.32121858      
[[step    13620]]     [[train 3.1919s]]     loss: -2.44544912      [[val 0.8412s]]     loss: -2.31951292      
[[step    13640]]     [[train 3.2373s]]     loss: -2.44745089      [[val 0.8421s]]     loss: -2.32080226      
[[step    13660]]     [[train 3.2115s]]     loss: -2.44681787      [[val 0.8351s]]     loss: -2.31936776      
[[step    13680]]     [[train 3.1506s]]     loss: -2.45234251      [[val 0.8179s]]     loss: -2.31714536      
[[step    13700]]     [[train 3.1396s]]     loss: -2.45137212      [[val 0.8045s]]     loss: -2.31957685      
[[step    13720]]     [[train 3.1114s]]     loss: -2.45480098      [[val 0.8068s]]     loss: -2.3165908       
[[step    13740]]     [[train 3.0657s]]     loss: -2.45667257      [[val 0.7977s]]     loss: -2.31520902      
[[step    13760]]     [[train 3.0682s]]     loss: -2.45747494      [[val 0.7878s]]     loss: -2.31630571      
[[step    13780]]     [[train 3.1128s]]     loss: -2.44839275      [[val 0.7856s]]     loss: -2.31733662      
[[step    13800]]     [[train 3.0995s]]     loss: -2.45257875      [[val 0.7941s]]     loss: -2.31627854      
[[step    13820]]     [[train 3.1095s]]     loss: -2.45658066      [[val 0.7795s]]     loss: -2.32185974      
[[step    13840]]     [[train 3.101s]]     loss: -2.45023553      [[val 0.7833s]]     loss: -2.31805345      
[[step    13860]]     [[train 3.0991s]]     loss: -2.45101789      [[val 0.792s]]     loss: -2.3180002       
[[step    13880]]     [[train 3.0865s]]     loss: -2.45718087      [[val 0.8006s]]     loss: -2.31939313      
[[step    13900]]     [[train 3.0774s]]     loss: -2.45317233      [[val 0.7974s]]     loss: -2.3207375       
[[step    13920]]     [[train 3.0329s]]     loss: -2.44975398      [[val 0.7966s]]     loss: -2.31907871      
[[step    13940]]     [[train 3.0349s]]     loss: -2.44877412      [[val 0.7916s]]     loss: -2.32351929      
[[step    13960]]     [[train 3.062s]]     loss: -2.4491254       [[val 0.7968s]]     loss: -2.32142539      
[[step    13980]]     [[train 3.1525s]]     loss: -2.45515403      [[val 0.8199s]]     loss: -2.32240751      
[[step    14000]]     [[train 3.2737s]]     loss: -2.45812217      [[val 0.8337s]]     loss: -2.31901875      
[[step    14020]]     [[train 3.4333s]]     loss: -2.46279012      [[val 0.8629s]]     loss: -2.31836204      
[[step    14040]]     [[train 3.5035s]]     loss: -2.47331805      [[val 0.8839s]]     loss: -2.3192484       
[[step    14060]]     [[train 3.5191s]]     loss: -2.47057015      [[val 0.8927s]]     loss: -2.31560177      
[[step    14080]]     [[train 3.514s]]     loss: -2.46415236      [[val 0.8876s]]     loss: -2.31125589      
[[step    14100]]     [[train 3.5156s]]     loss: -2.46411598      [[val 0.8987s]]     loss: -2.31057369      
[[step    14120]]     [[train 3.4832s]]     loss: -2.46499185      [[val 0.894s]]     loss: -2.31398966      
[[step    14140]]     [[train 3.4744s]]     loss: -2.4571834       [[val 0.9036s]]     loss: -2.31180424      
[[step    14160]]     [[train 3.5647s]]     loss: -2.46081009      [[val 0.925s]]     loss: -2.3176557       
[[step    14180]]     [[train 3.6176s]]     loss: -2.45719919      [[val 0.9406s]]     loss: -2.31812781      
[[step    14200]]     [[train 3.7136s]]     loss: -2.45248833      [[val 0.9629s]]     loss: -2.32117053      
[[step    14220]]     [[train 3.7592s]]     loss: -2.44486089      [[val 0.9779s]]     loss: -2.31926259      
[[step    14240]]     [[train 3.8609s]]     loss: -2.44750624      [[val 0.9911s]]     loss: -2.32058529      
[[step    14260]]     [[train 3.9108s]]     loss: -2.4416915       [[val 1.0006s]]     loss: -2.3186032       
[[step    14280]]     [[train 3.8976s]]     loss: -2.44828595      [[val 0.9929s]]     loss: -2.32077525      
[[step    14300]]     [[train 3.8037s]]     loss: -2.45737071      [[val 0.9703s]]     loss: -2.32072906      
[[step    14320]]     [[train 3.6609s]]     loss: -2.46678953      [[val 0.9442s]]     loss: -2.31720988      
[[step    14340]]     [[train 3.5512s]]     loss: -2.47261532      [[val 0.9202s]]     loss: -2.31477853      
[[step    14360]]     [[train 3.3922s]]     loss: -2.47652108      [[val 0.8877s]]     loss: -2.31548713      
[[step    14380]]     [[train 3.3059s]]     loss: -2.47818065      [[val 0.8731s]]     loss: -2.31415037      
[[step    14400]]     [[train 3.2482s]]     loss: -2.47347944      [[val 0.8603s]]     loss: -2.31345917      
[[step    14420]]     [[train 3.3053s]]     loss: -2.47175404      [[val 0.867s]]     loss: -2.3138671       
[[step    14440]]     [[train 3.2752s]]     loss: -2.4701194       [[val 0.8624s]]     loss: -2.31773534      
[[step    14460]]     [[train 3.2897s]]     loss: -2.47208316      [[val 0.8655s]]     loss: -2.3157705       
[[step    14480]]     [[train 3.2544s]]     loss: -2.46072476      [[val 0.8655s]]     loss: -2.31739014      
[[step    14500]]     [[train 3.2449s]]     loss: -2.46104573      [[val 0.865s]]     loss: -2.31352503      
[[step    14520]]     [[train 3.2343s]]     loss: -2.46012778      [[val 0.8639s]]     loss: -2.32008606      
[[step    14540]]     [[train 3.236s]]     loss: -2.45703482      [[val 0.8651s]]     loss: -2.31489278      
[[step    14560]]     [[train 3.2145s]]     loss: -2.45651956      [[val 0.858s]]     loss: -2.32020045      
[[step    14580]]     [[train 3.255s]]     loss: -2.45986684      [[val 0.8623s]]     loss: -2.3180138       
[[step    14600]]     [[train 3.2312s]]     loss: -2.45913262      [[val 0.8553s]]     loss: -2.32701682      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step    14620]]     [[train 3.2269s]]     loss: -2.46216621      [[val 0.8538s]]     loss: -2.31560728      
[[step    14640]]     [[train 3.1893s]]     loss: -2.46959107      [[val 0.8385s]]     loss: -2.31833197      
[[step    14660]]     [[train 3.1731s]]     loss: -2.46788691      [[val 0.8239s]]     loss: -2.3160014       
[[step    14680]]     [[train 3.0827s]]     loss: -2.47737961      [[val 0.7979s]]     loss: -2.31620888      
[[step    14700]]     [[train 3.0893s]]     loss: -2.48250694      [[val 0.7823s]]     loss: -2.30935025      
[[step    14720]]     [[train 3.0134s]]     loss: -2.47606422      [[val 0.77s]]     loss: -2.31646166      
[[step    14740]]     [[train 2.9986s]]     loss: -2.47271285      [[val 0.7671s]]     loss: -2.31480069      
[[step    14760]]     [[train 3.0087s]]     loss: -2.47059549      [[val 0.7772s]]     loss: -2.31411806      
[[step    14780]]     [[train 3.0462s]]     loss: -2.46730123      [[val 0.7817s]]     loss: -2.31524618      
[[step    14800]]     [[train 3.0465s]]     loss: -2.45976643      [[val 0.793s]]     loss: -2.31817427      
[[step    14820]]     [[train 3.0702s]]     loss: -2.46794872      [[val 0.7888s]]     loss: -2.31905715      
[[step    14840]]     [[train 3.0721s]]     loss: -2.4625492       [[val 0.7887s]]     loss: -2.31924193      
[[step    14860]]     [[train 3.0472s]]     loss: -2.46995498      [[val 0.7852s]]     loss: -2.32287262      
[[step    14880]]     [[train 3.0098s]]     loss: -2.46538488      [[val 0.7871s]]     loss: -2.32109484      
[[step    14900]]     [[train 2.9706s]]     loss: -2.47049409      [[val 0.7848s]]     loss: -2.31803602      
[[step    14920]]     [[train 2.9754s]]     loss: -2.47281244      [[val 0.7847s]]     loss: -2.3164205       
[[step    14940]]     [[train 3.0111s]]     loss: -2.48328436      [[val 0.7903s]]     loss: -2.31530766      
[[step    14960]]     [[train 3.0133s]]     loss: -2.48151817      [[val 0.785s]]     loss: -2.31359196      
[[step    14980]]     [[train 3.0309s]]     loss: -2.48775426      [[val 0.7823s]]     loss: -2.31290012      
[[step    15000]]     [[train 3.06s]]     loss: -2.48994081      [[val 0.789s]]     loss: -2.31595835      
[[step    15020]]     [[train 3.0396s]]     loss: -2.4864328       [[val 0.7868s]]     loss: -2.31421613      
[[step    15040]]     [[train 3.0306s]]     loss: -2.47837597      [[val 0.7817s]]     loss: -2.3149927       
[[step    15060]]     [[train 3.0568s]]     loss: -2.47369615      [[val 0.7842s]]     loss: -2.30941807      
[[step    15080]]     [[train 3.0698s]]     loss: -2.47343106      [[val 0.7815s]]     loss: -2.31267519      
[[step    15100]]     [[train 3.0581s]]     loss: -2.46887612      [[val 0.7816s]]     loss: -2.30844749      
[[step    15120]]     [[train 3.0894s]]     loss: -2.46811093      [[val 0.7844s]]     loss: -2.30907306      
[[step    15140]]     [[train 3.0701s]]     loss: -2.46945592      [[val 0.7869s]]     loss: -2.31056352      
[[step    15160]]     [[train 3.0378s]]     loss: -2.47209081      [[val 0.7817s]]     loss: -2.31238329      
[[step    15180]]     [[train 3.0341s]]     loss: -2.47144252      [[val 0.7938s]]     loss: -2.3129267       
[[step    15200]]     [[train 3.0434s]]     loss: -2.47027603      [[val 0.7862s]]     loss: -2.31596622      
[[step    15220]]     [[train 3.0603s]]     loss: -2.47130729      [[val 0.7843s]]     loss: -2.31777336      
[[step    15240]]     [[train 3.0665s]]     loss: -2.47201418      [[val 0.7858s]]     loss: -2.31437224      
[[step    15260]]     [[train 3.0813s]]     loss: -2.48378036      [[val 0.7893s]]     loss: -2.31368577      
[[step    15280]]     [[train 3.1223s]]     loss: -2.48489173      [[val 0.7846s]]     loss: -2.31241329      
[[step    15300]]     [[train 3.1172s]]     loss: -2.48725069      [[val 0.7846s]]     loss: -2.31042605      
[[step    15320]]     [[train 3.0826s]]     loss: -2.4854445       [[val 0.7949s]]     loss: -2.30996897      
[[step    15340]]     [[train 3.1024s]]     loss: -2.48985122      [[val 0.7969s]]     loss: -2.31358191      
[[step    15360]]     [[train 3.1104s]]     loss: -2.48143082      [[val 0.7964s]]     loss: -2.31457179      
[[step    15380]]     [[train 3.0875s]]     loss: -2.48005469      [[val 0.792s]]     loss: -2.31632347      
[[step    15400]]     [[train 3.097s]]     loss: -2.47936409      [[val 0.794s]]     loss: -2.31731799      
[[step    15420]]     [[train 3.1134s]]     loss: -2.48212959      [[val 0.7933s]]     loss: -2.31455578      
[[step    15440]]     [[train 3.1158s]]     loss: -2.48079898      [[val 0.7885s]]     loss: -2.31346273      
[[step    15460]]     [[train 3.1174s]]     loss: -2.4794338       [[val 0.798s]]     loss: -2.31510247      
[[step    15480]]     [[train 3.1236s]]     loss: -2.48198212      [[val 0.8002s]]     loss: -2.30973887      
[[step    15500]]     [[train 3.131s]]     loss: -2.48418402      [[val 0.7965s]]     loss: -2.3121476       
[[step    15520]]     [[train 3.1549s]]     loss: -2.48337649      [[val 0.7941s]]     loss: -2.31374607      
[[step    15540]]     [[train 3.1302s]]     loss: -2.48319236      [[val 0.7895s]]     loss: -2.31677576      
[[step    15560]]     [[train 3.1133s]]     loss: -2.48685029      [[val 0.7779s]]     loss: -2.3158969       
[[step    15580]]     [[train 3.0945s]]     loss: -2.48955019      [[val 0.7834s]]     loss: -2.3139681       
[[step    15600]]     [[train 3.1016s]]     loss: -2.48703838      [[val 0.786s]]     loss: -2.31129112      
[[step    15620]]     [[train 3.0733s]]     loss: -2.49320064      [[val 0.7774s]]     loss: -2.31292411      
[[step    15640]]     [[train 3.1036s]]     loss: -2.49820282      [[val 0.788s]]     loss: -2.30651956      
[[step    15660]]     [[train 3.1231s]]     loss: -2.49382057      [[val 0.7915s]]     loss: -2.30503031      
[[step    15680]]     [[train 3.1405s]]     loss: -2.48984364      [[val 0.7794s]]     loss: -2.30710447      
[[step    15700]]     [[train 3.1237s]]     loss: -2.48970907      [[val 0.7825s]]     loss: -2.30422173      
[[step    15720]]     [[train 3.1127s]]     loss: -2.48464308      [[val 0.7765s]]     loss: -2.30207237      
[[step    15740]]     [[train 3.0971s]]     loss: -2.47442552      [[val 0.7728s]]     loss: -2.30475343      
[[step    15760]]     [[train 3.1098s]]     loss: -2.47652878      [[val 0.776s]]     loss: -2.30454356      
[[step    15780]]     [[train 3.0824s]]     loss: -2.47793115      [[val 0.7782s]]     loss: -2.30802136      
[[step    15800]]     [[train 3.0723s]]     loss: -2.4821399       [[val 0.7781s]]     loss: -2.3088857       
[[step    15820]]     [[train 3.0893s]]     loss: -2.48672085      [[val 0.784s]]     loss: -2.31196625      
[[step    15840]]     [[train 3.0905s]]     loss: -2.49071764      [[val 0.7846s]]     loss: -2.3122867       
[[step    15860]]     [[train 3.0769s]]     loss: -2.49157807      [[val 0.7807s]]     loss: -2.31229041      
[[step    15880]]     [[train 3.0611s]]     loss: -2.50424816      [[val 0.7896s]]     loss: -2.30907248      
[[step    15900]]     [[train 3.0531s]]     loss: -2.50605202      [[val 0.7867s]]     loss: -2.30929033      
[[step    15920]]     [[train 3.0402s]]     loss: -2.49920311      [[val 0.7935s]]     loss: -2.30531147      
[[step    15940]]     [[train 3.0355s]]     loss: -2.50192355      [[val 0.7916s]]     loss: -2.30496166      
[[step    15960]]     [[train 3.0364s]]     loss: -2.50511236      [[val 0.7886s]]     loss: -2.30444606      
[[step    15980]]     [[train 3.082s]]     loss: -2.4949086       [[val 0.7831s]]     loss: -2.30633165      
[[step    16000]]     [[train 3.111s]]     loss: -2.49131515      [[val 0.7837s]]     loss: -2.3039588       
[[step    16020]]     [[train 3.1233s]]     loss: -2.4961733       [[val 0.7813s]]     loss: -2.30476818      
[[step    16040]]     [[train 3.1232s]]     loss: -2.49768979      [[val 0.7855s]]     loss: -2.30399735      
[[step    16060]]     [[train 3.1209s]]     loss: -2.50001841      [[val 0.7925s]]     loss: -2.304399        
[[step    16080]]     [[train 3.0937s]]     loss: -2.49545844      [[val 0.7935s]]     loss: -2.30630229      
[[step    16100]]     [[train 3.0772s]]     loss: -2.49556451      [[val 0.7926s]]     loss: -2.30968479      
[[step    16120]]     [[train 3.0846s]]     loss: -2.49239143      [[val 0.7967s]]     loss: -2.30897762      
restoring model from checkpoints/1564954698_ordertests_mean/model-14600
WARNING:tensorflow:From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
From /cluster/yn87erow/thesis/ext/handwriting-synthesis/venv_lme50/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
INFO:tensorflow:Restoring parameters from checkpoints/1564954698_ordertests_mean/model-14600
Restoring parameters from checkpoints/1564954698_ordertests_mean/model-14600
[[step    14620]]     [[train 3.132s]]     loss: -2.49208992      [[val 0.7887s]]     loss: -2.31477173      
[[step    14640]]     [[train 3.1704s]]     loss: -2.49709327      [[val 0.7848s]]     loss: -2.32255755      
[[step    14660]]     [[train 3.2286s]]     loss: -2.50821109      [[val 0.7856s]]     loss: -2.33038165      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step    14680]]     [[train 3.2859s]]     loss: -2.51761869      [[val 0.7852s]]     loss: -2.33467092      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step    14700]]     [[train 3.361s]]     loss: -2.53160769      [[val 0.7869s]]     loss: -2.34462916      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step    14720]]     [[train 3.351s]]     loss: -2.5418045       [[val 0.7857s]]     loss: -2.3475931       
saving model to checkpoints/1564954698_ordertests_mean/model
[[step    14740]]     [[train 3.4026s]]     loss: -2.53986403      [[val 0.7914s]]     loss: -2.34476084      
[[step    14760]]     [[train 3.4147s]]     loss: -2.54194503      [[val 0.7956s]]     loss: -2.34163759      
[[step    14780]]     [[train 3.4309s]]     loss: -2.54601776      [[val 0.7927s]]     loss: -2.3449256       
[[step    14800]]     [[train 3.3855s]]     loss: -2.54968027      [[val 0.7869s]]     loss: -2.34169473      
[[step    14820]]     [[train 3.3987s]]     loss: -2.54901703      [[val 0.7909s]]     loss: -2.34071501      
[[step    14840]]     [[train 3.3822s]]     loss: -2.55046815      [[val 0.79s]]     loss: -2.34390822      
[[step    14860]]     [[train 3.5044s]]     loss: -2.55363054      [[val 0.8116s]]     loss: -2.34126285      
[[step    14880]]     [[train 3.5765s]]     loss: -2.55498085      [[val 0.8443s]]     loss: -2.34099013      
[[step    14900]]     [[train 3.7296s]]     loss: -2.54917503      [[val 0.8724s]]     loss: -2.34408962      
[[step    14920]]     [[train 3.84s]]     loss: -2.54999952      [[val 0.9053s]]     loss: -2.33995943      
[[step    14940]]     [[train 3.8894s]]     loss: -2.55143575      [[val 0.9156s]]     loss: -2.33861269      
[[step    14960]]     [[train 3.8578s]]     loss: -2.55184885      [[val 0.9197s]]     loss: -2.33901168      
[[step    14980]]     [[train 3.8767s]]     loss: -2.55372835      [[val 0.9228s]]     loss: -2.33876438      
[[step    15000]]     [[train 3.8504s]]     loss: -2.55863935      [[val 0.9215s]]     loss: -2.33764144      
[[step    15020]]     [[train 3.8166s]]     loss: -2.55775687      [[val 0.916s]]     loss: -2.33629884      
[[step    15040]]     [[train 3.8503s]]     loss: -2.55884289      [[val 0.9218s]]     loss: -2.3379902       
[[step    15060]]     [[train 3.861s]]     loss: -2.55619191      [[val 0.9203s]]     loss: -2.34116706      
[[step    15080]]     [[train 3.9057s]]     loss: -2.55515298      [[val 0.9286s]]     loss: -2.34074673      
[[step    15100]]     [[train 3.881s]]     loss: -2.55550995      [[val 0.9206s]]     loss: -2.3373379       
[[step    15120]]     [[train 3.9465s]]     loss: -2.56022984      [[val 0.9257s]]     loss: -2.33860891      
[[step    15140]]     [[train 3.9444s]]     loss: -2.56027717      [[val 0.9374s]]     loss: -2.33745082      
[[step    15160]]     [[train 3.9752s]]     loss: -2.56276885      [[val 0.9349s]]     loss: -2.33127259      
[[step    15180]]     [[train 3.8824s]]     loss: -2.5627568       [[val 0.911s]]     loss: -2.32916268      
[[step    15200]]     [[train 3.8048s]]     loss: -2.56334415      [[val 0.9001s]]     loss: -2.32846582      
[[step    15220]]     [[train 3.6686s]]     loss: -2.55851545      [[val 0.8619s]]     loss: -2.33187181      
[[step    15240]]     [[train 3.5801s]]     loss: -2.557683        [[val 0.8404s]]     loss: -2.3315607       
[[step    15260]]     [[train 3.4795s]]     loss: -2.56065162      [[val 0.8109s]]     loss: -2.33291656      
[[step    15280]]     [[train 3.4195s]]     loss: -2.56336142      [[val 0.8008s]]     loss: -2.33237591      
[[step    15300]]     [[train 3.4007s]]     loss: -2.56335458      [[val 0.7875s]]     loss: -2.33211348      
[[step    15320]]     [[train 3.4023s]]     loss: -2.56896886      [[val 0.7986s]]     loss: -2.32815159      
[[step    15340]]     [[train 3.4034s]]     loss: -2.57312771      [[val 0.79s]]     loss: -2.32484457      
[[step    15360]]     [[train 3.3818s]]     loss: -2.5693612       [[val 0.7928s]]     loss: -2.32738737      
[[step    15380]]     [[train 3.394s]]     loss: -2.56438136      [[val 0.79s]]     loss: -2.32729822      
[[step    15400]]     [[train 3.3854s]]     loss: -2.56115831      [[val 0.7939s]]     loss: -2.33015459      
[[step    15420]]     [[train 3.3992s]]     loss: -2.56059889      [[val 0.7962s]]     loss: -2.32865844      
[[step    15440]]     [[train 3.3799s]]     loss: -2.56283498      [[val 0.799s]]     loss: -2.32698175      
[[step    15460]]     [[train 3.3605s]]     loss: -2.56717727      [[val 0.7988s]]     loss: -2.32648467      
[[step    15480]]     [[train 3.3523s]]     loss: -2.57010752      [[val 0.8032s]]     loss: -2.32531585      
[[step    15500]]     [[train 3.3525s]]     loss: -2.57305036      [[val 0.8062s]]     loss: -2.32180865      
[[step    15520]]     [[train 3.3355s]]     loss: -2.56708753      [[val 0.7931s]]     loss: -2.3244781       
[[step    15540]]     [[train 3.3684s]]     loss: -2.56520463      [[val 0.797s]]     loss: -2.32698525      
[[step    15560]]     [[train 3.3759s]]     loss: -2.56433973      [[val 0.7984s]]     loss: -2.32233555      
[[step    15580]]     [[train 3.3673s]]     loss: -2.57035067      [[val 0.7916s]]     loss: -2.32494126      
[[step    15600]]     [[train 3.3975s]]     loss: -2.56960621      [[val 0.7897s]]     loss: -2.3226094       
[[step    15620]]     [[train 3.3965s]]     loss: -2.57620733      [[val 0.7933s]]     loss: -2.32155267      
[[step    15640]]     [[train 3.3793s]]     loss: -2.57582908      [[val 0.7993s]]     loss: -2.32287855      
[[step    15660]]     [[train 3.4008s]]     loss: -2.57193129      [[val 0.7957s]]     loss: -2.32218652      
[[step    15680]]     [[train 3.4158s]]     loss: -2.56664036      [[val 0.7935s]]     loss: -2.31830014      
[[step    15700]]     [[train 3.41s]]     loss: -2.56641137      [[val 0.8005s]]     loss: -2.32081507      
[[step    15720]]     [[train 3.3877s]]     loss: -2.56943081      [[val 0.8071s]]     loss: -2.31903076      
[[step    15740]]     [[train 3.3502s]]     loss: -2.57252167      [[val 0.7859s]]     loss: -2.31680738      
restoring model from checkpoints/1564954698_ordertests_mean/model-14720
INFO:tensorflow:Restoring parameters from checkpoints/1564954698_ordertests_mean/model-14720
Restoring parameters from checkpoints/1564954698_ordertests_mean/model-14720
[[step    14740]]     [[train 3.3268s]]     loss: -2.57766285      [[val 0.7945s]]     loss: -2.32361499      
[[step    14760]]     [[train 3.3147s]]     loss: -2.57617272      [[val 0.798s]]     loss: -2.33016114      
[[step    14780]]     [[train 3.3133s]]     loss: -2.57713829      [[val 0.7977s]]     loss: -2.33569364      
[[step    14800]]     [[train 3.343s]]     loss: -2.57445508      [[val 0.7964s]]     loss: -2.34080662      
[[step    14820]]     [[train 3.3588s]]     loss: -2.57053275      [[val 0.8034s]]     loss: -2.34642352      
[[step    14840]]     [[train 3.3634s]]     loss: -2.56738348      [[val 0.8031s]]     loss: -2.34706855      
[[step    14860]]     [[train 3.3624s]]     loss: -2.56885922      [[val 0.803s]]     loss: -2.35112772      
saving model to checkpoints/1564954698_ordertests_mean/model
[[step    14880]]     [[train 3.3715s]]     loss: -2.57040433      [[val 0.7982s]]     loss: -2.34995744      
[[step    14900]]     [[train 3.3495s]]     loss: -2.57394307      [[val 0.7887s]]     loss: -2.34972579      
[[step    14920]]     [[train 3.333s]]     loss: -2.57595774      [[val 0.7881s]]     loss: -2.35063454      
[[step    14940]]     [[train 3.345s]]     loss: -2.57778346      [[val 0.7739s]]     loss: -2.34860288      
[[step    14960]]     [[train 3.3616s]]     loss: -2.57980738      [[val 0.7785s]]     loss: -2.34516006      
[[step    14980]]     [[train 3.3393s]]     loss: -2.57727877      [[val 0.7839s]]     loss: -2.34459223      
[[step    15000]]     [[train 3.3777s]]     loss: -2.57385583      [[val 0.7927s]]     loss: -2.34472408      
[[step    15020]]     [[train 3.4239s]]     loss: -2.57325994      [[val 0.7906s]]     loss: -2.34690871      
[[step    15040]]     [[train 3.3957s]]     loss: -2.5761072       [[val 0.8041s]]     loss: -2.34521103      
[[step    15060]]     [[train 3.3716s]]     loss: -2.57648351      [[val 0.7976s]]     loss: -2.34611016      
[[step    15080]]     [[train 3.3743s]]     loss: -2.57869058      [[val 0.7901s]]     loss: -2.34694701      
[[step    15100]]     [[train 3.3413s]]     loss: -2.57797685      [[val 0.7907s]]     loss: -2.34736204      
[[step    15120]]     [[train 3.2903s]]     loss: -2.58085184      [[val 0.7968s]]     loss: -2.34258761      
[[step    15140]]     [[train 3.2971s]]     loss: -2.57972552      [[val 0.793s]]     loss: -2.34462692      
[[step    15160]]     [[train 3.311s]]     loss: -2.57976953      [[val 0.7985s]]     loss: -2.34132343      
[[step    15180]]     [[train 3.3134s]]     loss: -2.58208045      [[val 0.7938s]]     loss: -2.34056849      
[[step    15200]]     [[train 3.3287s]]     loss: -2.5845447       [[val 0.7941s]]     loss: -2.34224526      
[[step    15220]]     [[train 3.3556s]]     loss: -2.58402191      [[val 0.7908s]]     loss: -2.34025338      
[[step    15240]]     [[train 3.3852s]]     loss: -2.58460752      [[val 0.7895s]]     loss: -2.34134131      
[[step    15260]]     [[train 3.3701s]]     loss: -2.58712439      [[val 0.785s]]     loss: -2.34044953      
[[step    15280]]     [[train 3.3576s]]     loss: -2.58825804      [[val 0.7922s]]     loss: -2.3394705       
[[step    15300]]     [[train 3.3344s]]     loss: -2.58543144      [[val 0.7976s]]     loss: -2.33824028      
[[step    15320]]     [[train 3.346s]]     loss: -2.58534792      [[val 0.8075s]]     loss: -2.33907408      
[[step    15340]]     [[train 3.3126s]]     loss: -2.58322145      [[val 0.8034s]]     loss: -2.33972123      
[[step    15360]]     [[train 3.3193s]]     loss: -2.58331172      [[val 0.8031s]]     loss: -2.34133965      
[[step    15380]]     [[train 3.3502s]]     loss: -2.58232829      [[val 0.8065s]]     loss: -2.34096665      
best validation loss of -2.351127722263336 at training step 14860
early stopping - ending training.
Namespace(dataset='../datasets/iam-online/ordertests_graves/mean/train', name='1564954698_ordertests_mean')
train size 10165
val size 536
test size 10701
